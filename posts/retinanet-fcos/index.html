<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>RetinaNet &amp; FCOS | Hzb&#39;s Study Blog</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="RetinaNet   简介 RetinaNet 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。
  网络结构 RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。
  历史问题   在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。
  在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。
  针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。
  针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。
    主要贡献 提出Focal Loss：解决one-stage算法中，样本不平衡和难易样本的问题。
  样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。">
    <meta name="generator" content="Hugo 0.88.0" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    
    
    
      

    

    
    
    <meta property="og:title" content="RetinaNet &amp; FCOS" />
<meta property="og:description" content="RetinaNet   简介 RetinaNet 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。
  网络结构 RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。
  历史问题   在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。
  在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。
  针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。
  针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。
    主要贡献 提出Focal Loss：解决one-stage算法中，样本不平衡和难易样本的问题。
  样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/retinanet-fcos/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-27T10:13:28+08:00" />
<meta property="article:modified_time" content="2022-10-27T10:13:28+08:00" />

<meta itemprop="name" content="RetinaNet &amp; FCOS">
<meta itemprop="description" content="RetinaNet   简介 RetinaNet 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。
  网络结构 RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。
  历史问题   在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。
  在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。
  针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。
  针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。
    主要贡献 提出Focal Loss：解决one-stage算法中，样本不平衡和难易样本的问题。
  样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。"><meta itemprop="datePublished" content="2022-10-27T10:13:28+08:00" />
<meta itemprop="dateModified" content="2022-10-27T10:13:28+08:00" />
<meta itemprop="wordCount" content="115">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RetinaNet &amp; FCOS"/>
<meta name="twitter:description" content="RetinaNet   简介 RetinaNet 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。
  网络结构 RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。
  历史问题   在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。
  在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。
  针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。
  针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。
    主要贡献 提出Focal Loss：解决one-stage算法中，样本不平衡和难易样本的问题。
  样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Hzb&#39;s Study Blog
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">RetinaNet &amp; FCOS</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2022-10-27T10:13:28+08:00">October 27, 2022</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="retinanet">RetinaNet</h2>
<ul>
<li>
<h4 id="简介">简介</h4>
<p><a href="https://arxiv.org/pdf/1708.02002.pdf">RetinaNet</a> 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。</p>
</li>
<li>
<h4 id="网络结构">网络结构</h4>
<p><img src="/images/retinanet1.jpg" alt="retinanet1"></p>
<p>RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。</p>
</li>
<li>
<h4 id="历史问题">历史问题</h4>
<p><img src="/images/retinanet2.jpg" alt="retinanet2"></p>
<ul>
<li>
<p>在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。</p>
</li>
<li>
<p>在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。</p>
</li>
<li>
<p>针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。</p>
</li>
<li>
<p>针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。</p>
</li>
</ul>
</li>
<li>
<h4 id="主要贡献">主要贡献</h4>
<p>提出<strong>Focal Loss</strong>：解决one-stage算法中，样本不平衡和难易样本的问题。</p>
<ul>
<li>
<p>样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。</p>
</li>
<li>
<p>难易样本：希望模型更关注难分样本，难分样本在loss中的比重更大。</p>
</li>
</ul>
</li>
<li>
<h4 id="focal-loss">Focal Loss</h4>
<ul>
<li>
<p>二分类交叉熵损失函数：</p>
<p><img src="/images/retinanet3.jpg" alt="retinanet3"></p>
</li>
<li>
<p>解决正负样本不均：加上一个权重α（范围[0,1]）</p>
<p><img src="/images/retinanet5.jpg" alt="retinanet5"></p>
</li>
<li>
<p>解决难易样本不均：引入因子γ(≥0)，将高置信度的样本的损失权重降低</p>
<p><img src="/images/retinanet8.png" alt="retinanet8"></p>
</li>
<li>
<p>Focal Loss</p>
<p><img src="/images/retinanet6.jpg" alt="retinanet6"></p>
<p>展开后为</p>
<p><img src="/images/retinanet7.jpg" alt="retinanet7"></p>
</li>
</ul>
</li>
<li>
<h4 id="优缺点分析">优缺点分析</h4>
<ul>
<li>
<p>优点：提升准确率，降低了正负样本和难易样本不均衡带来的影响；</p>
</li>
<li>
<p>缺点：Focal Loss易受噪声干扰，对图像标注的准确性要求非常高，一旦有标错的样本，就会被focal loss当做困难样本，从而影响学习效果。</p>
</li>
</ul>
</li>
</ul>
<h2 id="fcos">FCOS</h2>
<ul>
<li>
<h4 id="简介-1">简介</h4>
<p><a href="https://arxiv.org/pdf/2006.09214.pdf">FCOS</a> 发表于ICCV 2019，是一阶段anchor free目标检测算法，其主要的卖点为无锚。通过回归特征图上每个位置距离目标框的上下左右距离来实现目标检测。如果一个位置落在了多个目标框内，文中的方法是通过多尺度+回归幅度限制的方法来缓解这个问题。为了解决目标框数量过多的问题，文中提出了center-ness的方法，为每个位置学习一个center-ness分数，最后乘以预测类别分数作为非极大抑制的输入参数来解决这个问题。</p>
</li>
<li>
<h4 id="核心思想">核心思想</h4>
<p>将铺设锚框变为铺设锚点，进行物体检测。</p>
<p>所谓铺设锚框，又称为Anchor-Based，是指在输出特征图上的每个像素的位置，放置几个预先定义的anchor框，在网络训练过程中，对这些anchor框进行分类与回归。
通过GT框和这些anchor框计算IoU，依据设定的阈值条件来定义正负样本，典型的有YOLOv3。</p>
<p>所谓铺设锚点，又称为Anchor-free，如下图所示，将原有的 对锚框进行分类与回归，变为对锚点进行分类与回归，其中回归是预测锚点到检测（GT）框上下左右四条边界的距离，典型的有FCOS。</p>
</li>
<li>
<h4 id="anchor-based缺点">Anchor-Based缺点</h4>
<ul>
<li>Anchor-Based方式，检测性能对于anchor的大小、数量、长宽比 都非常敏感，通过改变这些超参数Retinanet在COCO benchmark上面提升了4%的MAP。</li>
<li>固定size和aspect ratio的anchor损害了检测器的普适性，导致对于不同任务，anchor可能需要重新设置大小和长宽比。</li>
<li>为了达到更高的召回率（查全率），需要生成大量的anchor（FPN约18万个），但是大部分的anchor在训练时标记为负样本（negative），造成了样本不均衡问题。</li>
<li>在训练中，需要计算所有anchor与真实框的IoU，这样会消耗大量内存和计算资源。</li>
</ul>
</li>
<li>
<h4 id="fcos优点">FCOS优点</h4>
<ul>
<li>检测问题被统一到 FCN-solvable 的问题，可以简单地重用其他任务的idea，如语义分割。</li>
<li>anchor-free方式，不需要像anchor-based那样大量调整参数，使训练更为简单。</li>
<li>由于不需要计算IoU，节省了大量算力和内存。</li>
<li>提出了一些关于交叠区域的解决方法和思考。</li>
<li>模型部署会受到两种限制，一种是计算量的限制，一种是I/O 带宽的限制。anchor-free方式相比于anchor-based方式，对部署更友好一些。</li>
</ul>
</li>
<li>
<h4 id="网络结构-1">网络结构</h4>
<p><img src="/images/fcos1.jpg" alt="fcos1" style="zoom:67%;" /></p>
<p>FCOS采用FPN结构，backbone的C3、C4、C5特征层作为FPN的输入，FPN生成P3、P4、P5、P6、P7特征图，送入后续的检测头Head。</p>
<p>每个Head包含3个分支：</p>
<ul>
<li>classification分支：预测类别，图中的C表示类别数，相当于C个二分类；</li>
<li>regression分支：回归位置，图中的4表示：l、t、r、b，预测锚点到检测框上下左右四条边界的距离；</li>
<li>center-ness：中心度，一个锚点对应一个中心度，用于锚点相对于检测框中心性的判断
在检测子网络Head中，分类分支和回归分支都先经过了4个卷积层进行了特征强化。</li>
</ul>
<p>早期版本，在分类分支中，既包含 正、负样本锚点的 类别预测分支，又包含正、负样本锚点中心性判断的center-ness分支，用来强化检测结果；</p>
<p>回归分支用来回归正样本锚点到检测框上、下、左、右四个边界的距离 。
<img src="/images/fcos2.jpg" alt="fcos2" style="zoom: 80%;" /></p>
</li>
<li>
<h4 id="损失函数">损失函数</h4>
<p><img src="/images/fcos3.jpg" alt="fcos3" style="zoom:67%;" /></p>
</li>
</ul>
<h2 id="citation">Citation</h2>
<blockquote>
<p>北信科视觉感知研讨课程（高丹阳师姐分享）</p>
<p><a href="https://www.jianshu.com/p/4dbf876d1fae">RetinaNet</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/410436667">一阶段目标检测器-RetinaNet网络详解</a></p>
<p><a href="https://blog.csdn.net/zl_Dream/article/details/123730886">目标检测FCOS的初步理解</a></p>
<p><a href="https://blog.csdn.net/weixin_45377629/article/details/124844405">FCOS理论知识讲解</a></p>
</blockquote>
<h1 id="thanks-for-reading"><em>Thanks for reading!</em></h1>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy;  Hzb's Study Blog 2022 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
