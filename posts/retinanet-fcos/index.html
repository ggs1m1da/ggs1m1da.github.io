<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>RetinaNet &amp; FCOS | Hzb&#39;s Study Blog</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="RetinaNet 简介 RetinaNet 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。
网络结构 RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。
历史问题 在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。
在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。
针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。
针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。
主要贡献 提出Focal Loss：解决one-stage算法中，样本不平衡和难易样本的问题。
样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。
难易样本：希望模型更关注难分样本，难分样本在loss中的比重更大。
Focal Loss 二分类交叉熵损失函数：
解决正负样本不均：加上一个权重α（范围[0,1]）
解决难易样本不均：引入因子γ(≥0)，将高置信度的样本的损失权重降低
Focal Loss
展开后为
优缺点分析 优点：提升准确率，降低了正负样本和难易样本不均衡带来的影响；
缺点：Focal Loss易受噪声干扰，对图像标注的准确性要求非常高，一旦有标错的样本，就会被focal loss当做困难样本，从而影响学习效果。
Citation 北信科视觉感知研讨课程（高丹阳师姐分享）
RetinaNet
一阶段目标检测器-RetinaNet网络详解
Thanks for reading!">
    <meta name="generator" content="Hugo 0.101.0" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    
    
    
      

    

    
    
    <meta property="og:title" content="RetinaNet &amp; FCOS" />
<meta property="og:description" content="RetinaNet 简介 RetinaNet 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。
网络结构 RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。
历史问题 在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。
在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。
针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。
针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。
主要贡献 提出Focal Loss：解决one-stage算法中，样本不平衡和难易样本的问题。
样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。
难易样本：希望模型更关注难分样本，难分样本在loss中的比重更大。
Focal Loss 二分类交叉熵损失函数：
解决正负样本不均：加上一个权重α（范围[0,1]）
解决难易样本不均：引入因子γ(≥0)，将高置信度的样本的损失权重降低
Focal Loss
展开后为
优缺点分析 优点：提升准确率，降低了正负样本和难易样本不均衡带来的影响；
缺点：Focal Loss易受噪声干扰，对图像标注的准确性要求非常高，一旦有标错的样本，就会被focal loss当做困难样本，从而影响学习效果。
Citation 北信科视觉感知研讨课程（高丹阳师姐分享）
RetinaNet
一阶段目标检测器-RetinaNet网络详解
Thanks for reading!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/retinanet-fcos/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-10-27T10:13:28+08:00" />
<meta property="article:modified_time" content="2022-10-27T10:13:28+08:00" />

<meta itemprop="name" content="RetinaNet &amp; FCOS">
<meta itemprop="description" content="RetinaNet 简介 RetinaNet 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。
网络结构 RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。
历史问题 在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。
在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。
针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。
针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。
主要贡献 提出Focal Loss：解决one-stage算法中，样本不平衡和难易样本的问题。
样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。
难易样本：希望模型更关注难分样本，难分样本在loss中的比重更大。
Focal Loss 二分类交叉熵损失函数：
解决正负样本不均：加上一个权重α（范围[0,1]）
解决难易样本不均：引入因子γ(≥0)，将高置信度的样本的损失权重降低
Focal Loss
展开后为
优缺点分析 优点：提升准确率，降低了正负样本和难易样本不均衡带来的影响；
缺点：Focal Loss易受噪声干扰，对图像标注的准确性要求非常高，一旦有标错的样本，就会被focal loss当做困难样本，从而影响学习效果。
Citation 北信科视觉感知研讨课程（高丹阳师姐分享）
RetinaNet
一阶段目标检测器-RetinaNet网络详解
Thanks for reading!"><meta itemprop="datePublished" content="2022-10-27T10:13:28+08:00" />
<meta itemprop="dateModified" content="2022-10-27T10:13:28+08:00" />
<meta itemprop="wordCount" content="71">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RetinaNet &amp; FCOS"/>
<meta name="twitter:description" content="RetinaNet 简介 RetinaNet 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。
网络结构 RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。
历史问题 在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。
在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。
针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。
针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。
主要贡献 提出Focal Loss：解决one-stage算法中，样本不平衡和难易样本的问题。
样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。
难易样本：希望模型更关注难分样本，难分样本在loss中的比重更大。
Focal Loss 二分类交叉熵损失函数：
解决正负样本不均：加上一个权重α（范围[0,1]）
解决难易样本不均：引入因子γ(≥0)，将高置信度的样本的损失权重降低
Focal Loss
展开后为
优缺点分析 优点：提升准确率，降低了正负样本和难易样本不均衡带来的影响；
缺点：Focal Loss易受噪声干扰，对图像标注的准确性要求非常高，一旦有标错的样本，就会被focal loss当做困难样本，从而影响学习效果。
Citation 北信科视觉感知研讨课程（高丹阳师姐分享）
RetinaNet
一阶段目标检测器-RetinaNet网络详解
Thanks for reading!"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Hzb&#39;s Study Blog
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">RetinaNet &amp; FCOS</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2022-10-27T10:13:28+08:00">October 27, 2022</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="retinanet">RetinaNet</h2>
<ul>
<li>
<h4 id="简介">简介</h4>
<p><a href="https://arxiv.org/pdf/1708.02002.pdf">RetinaNet</a> 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。</p>
</li>
<li>
<h4 id="网络结构">网络结构</h4>
<p><img src="C:%5CUsers%5Chuzhanbin%5CDesktop%5Cswap%5Cblog%5Chzb_blog%5Cstatic%5Cimages%5Cretinanet1.jpg" alt="retinanet1"></p>
<p>RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。</p>
</li>
<li>
<h4 id="历史问题">历史问题</h4>
<p><img src="C:%5CUsers%5Chuzhanbin%5CDesktop%5Cswap%5Cblog%5Chzb_blog%5Cstatic%5Cimages%5Cretinanet2.jpg" alt="retinanet2"></p>
<ul>
<li>
<p>在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。</p>
</li>
<li>
<p>在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。</p>
</li>
<li>
<p>针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。</p>
</li>
<li>
<p>针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。</p>
</li>
</ul>
</li>
<li>
<h4 id="主要贡献">主要贡献</h4>
<p>提出<strong>Focal Loss</strong>：解决one-stage算法中，样本不平衡和难易样本的问题。</p>
<ul>
<li>
<p>样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。</p>
</li>
<li>
<p>难易样本：希望模型更关注难分样本，难分样本在loss中的比重更大。</p>
</li>
</ul>
</li>
<li>
<h4 id="focal-loss">Focal Loss</h4>
<ul>
<li>
<p>二分类交叉熵损失函数：</p>
<p><img src="C:%5CUsers%5Chuzhanbin%5CDesktop%5Cswap%5Cblog%5Chzb_blog%5Cstatic%5Cimages%5Cretinanet3.jpg" alt="retinanet3"></p>
</li>
<li>
<p>解决正负样本不均：加上一个权重α（范围[0,1]）</p>
<p><img src="C:%5CUsers%5Chuzhanbin%5CDesktop%5Cswap%5Cblog%5Chzb_blog%5Cstatic%5Cimages%5Cretinanet5.jpg" alt="retinanet5"></p>
</li>
<li>
<p>解决难易样本不均：引入因子γ(≥0)，将高置信度的样本的损失权重降低</p>
<p><img src="C:%5CUsers%5Chuzhanbin%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20221027111409635.png" alt="image-20221027111409635"></p>
</li>
<li>
<p>Focal Loss</p>
<p><img src="C:%5CUsers%5Chuzhanbin%5CDesktop%5Cswap%5Cblog%5Chzb_blog%5Cstatic%5Cimages%5Cretinanet6.jpg" alt="retinanet6"></p>
<p>展开后为</p>
<p><img src="C:%5CUsers%5Chuzhanbin%5CDesktop%5Cswap%5Cblog%5Chzb_blog%5Cstatic%5Cimages%5Cretinanet7.jpg" alt="retinanet7"></p>
</li>
</ul>
</li>
<li>
<h4 id="优缺点分析">优缺点分析</h4>
<ul>
<li>
<p>优点：提升准确率，降低了正负样本和难易样本不均衡带来的影响；</p>
</li>
<li>
<p>缺点：Focal Loss易受噪声干扰，对图像标注的准确性要求非常高，一旦有标错的样本，就会被focal loss当做困难样本，从而影响学习效果。</p>
</li>
</ul>
</li>
</ul>
<h2 id="citation">Citation</h2>
<blockquote>
<p>北信科视觉感知研讨课程（高丹阳师姐分享）</p>
<p><a href="https://www.jianshu.com/p/4dbf876d1fae">RetinaNet</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/410436667">一阶段目标检测器-RetinaNet网络详解</a></p>
</blockquote>
<h1 id="thanks-for-reading"><em>Thanks for reading!</em></h1>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy;  Hzb's Study Blog 2022 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
