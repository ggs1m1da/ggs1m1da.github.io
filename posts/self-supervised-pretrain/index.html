<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Self-Supervised PreTrain | Hzb&#39;s Study Blog</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Self-Supervised PreTrain Pretext task Pretext task也叫surrogate task，也称作代理任务。
Pretext可以理解为是一种为达到特定训练任务而设计的间接任务。比如，我们要训练一个网络来对ImageNet分类，可以表达为 fθ(x): x→y，我们的目的其实是获得具有语义特征提取/推理能力的 θ。我们假设有另外一个任务（也就是pretext），它可以近似获得这样的θ，比如，Auto-encoder（AE），表示为：gθ(x): x→x。为什么AE可以近似 θ呢？因为AE要重建 x 就必须学习 x 中的内在关系，而这种内在关系的学习又是有利于我们学习 fθ(x) 的。这种方式也叫做预训练，为了在目标任务上获得更好的泛化能力，一般还需要进行fine-tuning等操作。
MAE Fine-tuning   linear probe
linear probe不对训练好的模型做变动，只是用它从下游数据集提取特征，然后利用逻辑回归拟合标签；
  fine-tune
fine-tune利用下游数据再次微调预模型中的所有参数。
  CAE (a)CAE；(b)BEiT；(c)DAE
  encoder 仅用来做表征，将patches映射到一个表征空间。
  latent contextual regressor 用来通过使用encoder表征的可视patches来预测masked patches的表征，并添加一个alignment constraint来与使用encoder表征的masked patches进行对齐。
这一部分的作用使得CAE的encoder承担了全部的表征学习能力。
  decoder 将masked patches的表征映射回targets。
  Contrastive Learning 典型的对比学习的任务是通过解决最大化来自同一图像的增强视图之间的相似度以及最小化来自不同图像的增强图像之间的相似度来预训练一个网络。
对于ImageNet数据集，典型的对比学习主要学到了图像中心关于这一千个分类的知识，而MIM方法能够学到非中心区域的、这一千个分类以外的知识。">
    <meta name="generator" content="Hugo 0.88.0" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    
    
    
      

    

    
    
    <meta property="og:title" content="Self-Supervised PreTrain" />
<meta property="og:description" content="Self-Supervised PreTrain Pretext task Pretext task也叫surrogate task，也称作代理任务。
Pretext可以理解为是一种为达到特定训练任务而设计的间接任务。比如，我们要训练一个网络来对ImageNet分类，可以表达为 fθ(x): x→y，我们的目的其实是获得具有语义特征提取/推理能力的 θ。我们假设有另外一个任务（也就是pretext），它可以近似获得这样的θ，比如，Auto-encoder（AE），表示为：gθ(x): x→x。为什么AE可以近似 θ呢？因为AE要重建 x 就必须学习 x 中的内在关系，而这种内在关系的学习又是有利于我们学习 fθ(x) 的。这种方式也叫做预训练，为了在目标任务上获得更好的泛化能力，一般还需要进行fine-tuning等操作。
MAE Fine-tuning   linear probe
linear probe不对训练好的模型做变动，只是用它从下游数据集提取特征，然后利用逻辑回归拟合标签；
  fine-tune
fine-tune利用下游数据再次微调预模型中的所有参数。
  CAE (a)CAE；(b)BEiT；(c)DAE
  encoder 仅用来做表征，将patches映射到一个表征空间。
  latent contextual regressor 用来通过使用encoder表征的可视patches来预测masked patches的表征，并添加一个alignment constraint来与使用encoder表征的masked patches进行对齐。
这一部分的作用使得CAE的encoder承担了全部的表征学习能力。
  decoder 将masked patches的表征映射回targets。
  Contrastive Learning 典型的对比学习的任务是通过解决最大化来自同一图像的增强视图之间的相似度以及最小化来自不同图像的增强图像之间的相似度来预训练一个网络。
对于ImageNet数据集，典型的对比学习主要学到了图像中心关于这一千个分类的知识，而MIM方法能够学到非中心区域的、这一千个分类以外的知识。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/self-supervised-pretrain/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-20T11:23:46+08:00" />
<meta property="article:modified_time" content="2022-11-20T11:23:46+08:00" />

<meta itemprop="name" content="Self-Supervised PreTrain">
<meta itemprop="description" content="Self-Supervised PreTrain Pretext task Pretext task也叫surrogate task，也称作代理任务。
Pretext可以理解为是一种为达到特定训练任务而设计的间接任务。比如，我们要训练一个网络来对ImageNet分类，可以表达为 fθ(x): x→y，我们的目的其实是获得具有语义特征提取/推理能力的 θ。我们假设有另外一个任务（也就是pretext），它可以近似获得这样的θ，比如，Auto-encoder（AE），表示为：gθ(x): x→x。为什么AE可以近似 θ呢？因为AE要重建 x 就必须学习 x 中的内在关系，而这种内在关系的学习又是有利于我们学习 fθ(x) 的。这种方式也叫做预训练，为了在目标任务上获得更好的泛化能力，一般还需要进行fine-tuning等操作。
MAE Fine-tuning   linear probe
linear probe不对训练好的模型做变动，只是用它从下游数据集提取特征，然后利用逻辑回归拟合标签；
  fine-tune
fine-tune利用下游数据再次微调预模型中的所有参数。
  CAE (a)CAE；(b)BEiT；(c)DAE
  encoder 仅用来做表征，将patches映射到一个表征空间。
  latent contextual regressor 用来通过使用encoder表征的可视patches来预测masked patches的表征，并添加一个alignment constraint来与使用encoder表征的masked patches进行对齐。
这一部分的作用使得CAE的encoder承担了全部的表征学习能力。
  decoder 将masked patches的表征映射回targets。
  Contrastive Learning 典型的对比学习的任务是通过解决最大化来自同一图像的增强视图之间的相似度以及最小化来自不同图像的增强图像之间的相似度来预训练一个网络。
对于ImageNet数据集，典型的对比学习主要学到了图像中心关于这一千个分类的知识，而MIM方法能够学到非中心区域的、这一千个分类以外的知识。"><meta itemprop="datePublished" content="2022-11-20T11:23:46+08:00" />
<meta itemprop="dateModified" content="2022-11-20T11:23:46+08:00" />
<meta itemprop="wordCount" content="46">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Self-Supervised PreTrain"/>
<meta name="twitter:description" content="Self-Supervised PreTrain Pretext task Pretext task也叫surrogate task，也称作代理任务。
Pretext可以理解为是一种为达到特定训练任务而设计的间接任务。比如，我们要训练一个网络来对ImageNet分类，可以表达为 fθ(x): x→y，我们的目的其实是获得具有语义特征提取/推理能力的 θ。我们假设有另外一个任务（也就是pretext），它可以近似获得这样的θ，比如，Auto-encoder（AE），表示为：gθ(x): x→x。为什么AE可以近似 θ呢？因为AE要重建 x 就必须学习 x 中的内在关系，而这种内在关系的学习又是有利于我们学习 fθ(x) 的。这种方式也叫做预训练，为了在目标任务上获得更好的泛化能力，一般还需要进行fine-tuning等操作。
MAE Fine-tuning   linear probe
linear probe不对训练好的模型做变动，只是用它从下游数据集提取特征，然后利用逻辑回归拟合标签；
  fine-tune
fine-tune利用下游数据再次微调预模型中的所有参数。
  CAE (a)CAE；(b)BEiT；(c)DAE
  encoder 仅用来做表征，将patches映射到一个表征空间。
  latent contextual regressor 用来通过使用encoder表征的可视patches来预测masked patches的表征，并添加一个alignment constraint来与使用encoder表征的masked patches进行对齐。
这一部分的作用使得CAE的encoder承担了全部的表征学习能力。
  decoder 将masked patches的表征映射回targets。
  Contrastive Learning 典型的对比学习的任务是通过解决最大化来自同一图像的增强视图之间的相似度以及最小化来自不同图像的增强图像之间的相似度来预训练一个网络。
对于ImageNet数据集，典型的对比学习主要学到了图像中心关于这一千个分类的知识，而MIM方法能够学到非中心区域的、这一千个分类以外的知识。"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Hzb&#39;s Study Blog
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Self-Supervised PreTrain</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2022-11-20T11:23:46+08:00">November 20, 2022</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h1 id="self-supervised-pretrain">Self-Supervised PreTrain</h1>
<h2 id="pretext-task">Pretext task</h2>
<p>Pretext task也叫surrogate task，也称作代理任务。</p>
<p>Pretext可以理解为是一种为达到特定训练任务而设计的间接任务。比如，我们要训练一个网络来对ImageNet分类，可以表达为 fθ(x): x→y，我们的目的其实是获得具有语义特征提取/推理能力的 θ。我们假设有另外一个任务（也就是pretext），它可以近似获得这样的θ，比如，Auto-encoder（AE），表示为：gθ(x): x→x。为什么AE可以近似 θ呢？因为AE要重建 x 就必须学习 x 中的内在关系，而这种内在关系的学习又是有利于我们学习 fθ(x) 的。这种方式也叫做预训练，为了在目标任务上获得更好的泛化能力，一般还需要进行fine-tuning等操作。</p>
<h2 id="mae">MAE</h2>
<h4 id="fine-tuning">Fine-tuning</h4>
<ul>
<li>
<p>linear probe</p>
<p>linear probe不对训练好的模型做变动，只是用它从下游数据集提取特征，然后利用逻辑回归拟合标签；</p>
</li>
<li>
<p>fine-tune</p>
<p>fine-tune利用下游数据再次微调预模型中的所有参数。</p>
</li>
</ul>
<h2 id="cae">CAE</h2>
<p><img src="C:%5CUsers%5C10426%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20221114211147841.png" alt="image-20221114211147841"></p>
<p>(a)CAE；(b)BEiT；(c)DAE</p>
<ul>
<li>
<h4 id="encoder">encoder</h4>
<p>仅用来做表征，将patches映射到一个表征空间。</p>
</li>
<li>
<h4 id="latent-contextual-regressor">latent contextual regressor</h4>
<p>用来通过使用encoder表征的可视patches来预测masked patches的表征，并添加一个alignment constraint来与使用encoder表征的masked patches进行对齐。</p>
<p>这一部分的作用使得CAE的encoder承担了全部的表征学习能力。</p>
</li>
<li>
<h4 id="decoder">decoder</h4>
<p>将masked patches的表征映射回targets。</p>
</li>
</ul>
<h2 id="contrastive-learning">Contrastive Learning</h2>
<p>典型的对比学习的任务是通过解决最大化来自同一图像的增强视图之间的相似度以及最小化来自不同图像的增强图像之间的相似度来预训练一个网络。</p>
<p>对于ImageNet数据集，典型的对比学习主要学到了图像中心关于这一千个分类的知识，而MIM方法能够学到非中心区域的、这一千个分类以外的知识。</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy;  Hzb's Study Blog 2022 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
