<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Resnet | Hzb&#39;s Study Blog</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="ResNet 简介 ResNet网络在 2015年由微软实验室中的何凯明等人提出，斩获当年ImageNet竞赛中分类任务第一名，目标检测第一名。获得COCO数据集中目标检测第一名，图像分割第一名。
亮点 超深的网络结构（超过1000层） 提出residual（残差结构）模块 使用Batch Normalization加速训练（丢弃Dropout） 为什么采用residual 在ResNet提出之前，所有的神经网络都是通过卷积层和池化层的叠加组成的。人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实际的试验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而出现两种问题： 梯度消失和梯度爆炸。梯度消失：若每一层的误差梯度小于1，反向传播时，网络越深，梯度越趋近于0；梯度爆炸：若每一层的误差梯度大于1，反向传播时，网络越深，梯度越来越大。 退化问题。随着层数的增加，预测效果反而越来越差。 ">
    <meta name="generator" content="Hugo 0.101.0" />
    
    
    
    
      <meta name="robots" content="noindex, nofollow">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    
    
    
      

    

    
    
    <meta property="og:title" content="Resnet" />
<meta property="og:description" content="ResNet 简介 ResNet网络在 2015年由微软实验室中的何凯明等人提出，斩获当年ImageNet竞赛中分类任务第一名，目标检测第一名。获得COCO数据集中目标检测第一名，图像分割第一名。
亮点 超深的网络结构（超过1000层） 提出residual（残差结构）模块 使用Batch Normalization加速训练（丢弃Dropout） 为什么采用residual 在ResNet提出之前，所有的神经网络都是通过卷积层和池化层的叠加组成的。人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实际的试验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而出现两种问题： 梯度消失和梯度爆炸。梯度消失：若每一层的误差梯度小于1，反向传播时，网络越深，梯度越趋近于0；梯度爆炸：若每一层的误差梯度大于1，反向传播时，网络越深，梯度越来越大。 退化问题。随着层数的增加，预测效果反而越来越差。 " />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/posts/resnet/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-26T10:54:08+08:00" />
<meta property="article:modified_time" content="2022-08-26T10:54:08+08:00" />

<meta itemprop="name" content="Resnet">
<meta itemprop="description" content="ResNet 简介 ResNet网络在 2015年由微软实验室中的何凯明等人提出，斩获当年ImageNet竞赛中分类任务第一名，目标检测第一名。获得COCO数据集中目标检测第一名，图像分割第一名。
亮点 超深的网络结构（超过1000层） 提出residual（残差结构）模块 使用Batch Normalization加速训练（丢弃Dropout） 为什么采用residual 在ResNet提出之前，所有的神经网络都是通过卷积层和池化层的叠加组成的。人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实际的试验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而出现两种问题： 梯度消失和梯度爆炸。梯度消失：若每一层的误差梯度小于1，反向传播时，网络越深，梯度越趋近于0；梯度爆炸：若每一层的误差梯度大于1，反向传播时，网络越深，梯度越来越大。 退化问题。随着层数的增加，预测效果反而越来越差。 "><meta itemprop="datePublished" content="2022-08-26T10:54:08+08:00" />
<meta itemprop="dateModified" content="2022-08-26T10:54:08+08:00" />
<meta itemprop="wordCount" content="13">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Resnet"/>
<meta name="twitter:description" content="ResNet 简介 ResNet网络在 2015年由微软实验室中的何凯明等人提出，斩获当年ImageNet竞赛中分类任务第一名，目标检测第一名。获得COCO数据集中目标检测第一名，图像分割第一名。
亮点 超深的网络结构（超过1000层） 提出residual（残差结构）模块 使用Batch Normalization加速训练（丢弃Dropout） 为什么采用residual 在ResNet提出之前，所有的神经网络都是通过卷积层和池化层的叠加组成的。人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实际的试验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而出现两种问题： 梯度消失和梯度爆炸。梯度消失：若每一层的误差梯度小于1，反向传播时，网络越深，梯度越趋近于0；梯度爆炸：若每一层的误差梯度大于1，反向传播时，网络越深，梯度越来越大。 退化问题。随着层数的增加，预测效果反而越来越差。 "/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Hzb&#39;s Study Blog
      
    </a>
    <div class="flex-l items-center">
      

      
      
<div class="ananke-socials">
  
</div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      










  <div id="sharing" class="mt3 ananke-socials">
    
  </div>


      <h1 class="f1 athelas mt3 mb1">Resnet</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2022-08-26T10:54:08+08:00">August 26, 2022</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="resnet">ResNet</h2>
<ul>
<li>
<h4 id="简介">简介</h4>
<p>ResNet网络在 2015年由微软实验室中的何凯明等人提出，斩获当年ImageNet竞赛中分类任务第一名，目标检测第一名。获得COCO数据集中目标检测第一名，图像分割第一名。</p>
</li>
<li>
<h4 id="亮点">亮点</h4>
<ul>
<li>超深的网络结构（超过1000层）</li>
<li>提出residual（残差结构）模块</li>
<li>使用Batch Normalization加速训练（丢弃Dropout）</li>
</ul>
</li>
<li>
<h4 id="为什么采用residual">为什么采用residual</h4>
<ul>
<li>在ResNet提出之前，所有的神经网络都是通过卷积层和池化层的叠加组成的。人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实际的试验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而出现两种问题：
<ol>
<li>梯度消失和梯度爆炸。梯度消失：若每一层的误差梯度小于1，反向传播时，网络越深，梯度越趋近于0；梯度爆炸：若每一层的误差梯度大于1，反向传播时，网络越深，梯度越来越大。</li>
<li>退化问题。随着层数的增加，预测效果反而越来越差。</li>
</ol>
</li>
</ul>
</li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy;  Hzb's Study Blog 2022 
  </a>
    <div>
<div class="ananke-socials">
  
</div>
</div>
  </div>
</footer>

  </body>
</html>
