<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hzb&#39;s Study Blog</title>
    <link>http://example.org/</link>
    <description>Recent content on Hzb&#39;s Study Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 20 Aug 2022 18:00:30 +0800</lastBuildDate><atom:link href="http://example.org/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AlexNet &amp; VGG &amp; GoogLeNet</title>
      <link>http://example.org/posts/alexnet-vgg-googlenet/</link>
      <pubDate>Sat, 20 Aug 2022 18:00:30 +0800</pubDate>
      
      <guid>http://example.org/posts/alexnet-vgg-googlenet/</guid>
      <description>AlexNet 简介 AlexNet是2012年ISLVRC 2012（ImageNet Large Scale Visual Recognition Challenge）竞赛的冠军网络，分类准确率由传统的 70%+提升到 80%+。由Hinton和他的学生Alex Krizhevsky设计。也是在那年之后，深度学习开始迅速发展。
亮点 首次使用GPU进行模型训练。 使用了ReLU激活函数，而不是传统的Sigmoid激活函数以及Tanh激活函数。主要原因是ReLU函数在进行梯度下降的计算过程中能显著加快训练过程，也就是非饱和的线性的激活函数要快于sigmoid和tanh等饱和的非线性激活函数的收敛速度（在这篇论文中并没有谈及ReLU函数对于梯度消失问题的解决，只是从收敛速度上论述的）。 使用了LRN局部响应归一化（Local Response Normalization，LRN）。AlexNet中提出局部响应归一化层（LRN）,是跟在激活或池化之后的一种为了提高准确度的技术方法。LRN的主要思想就是对局部神经元的神经活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈更小的神经元，从而也在一定程度上增强了泛化能力。 在全连接层的前两层中使用了Dropout随机失活神经元操作，以减少过拟合。 模型架构 ReLU激活函数 针对sigmoid梯度饱和导致训练收敛慢的问题，在AlexNet中引入了ReLU。ReLU是一个分段线性函数，小于等于0则输出为0；大于0的则恒等输出。相比于sigmoid，ReLU有以下有特点：
计算开销小。sigmoid的正向传播有指数运算，倒数运算，而ReLu是线性输出；反向传播中，sigmoid有指数运算，而ReLU有输出的部分，导数始终为1。 梯度饱和问题。 稀疏性。ReLU会使一部跟神经元的输出为0，这样造就了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。 Dropout 过拟合的根本原因是特征维度过多，模型过于复杂，参数过多，训练数据过少，噪声过多，导致模型完美拟合训练集，但缺乏泛化能力，对新数据的测试集预测结果差。
Dropout的使用有效地缓解了模型复杂度提升导致的过拟合。Dropout通过修改神经网络本身结构来实现，对于某一层的神经元，通过定义的概率将神经元置为0，这个神经元就不参与前向和后向传播，就如同在网络中被删除了一样，同时保持输入层与输出层神经元的个数不变，然后按原有的学习方法进行参数更新。在下一次迭代中，又重新随机删除一些神经元（置为0），直至训练结束。
Dropout应该算是AlexNet中一个很大的创新，现在神经网络中的必备结构之一。Dropout也可以看成是一种模型组合，每次生成的网络结构都不一样，通过组合多个模型的方式能够有效地减少过拟合，Dropout只需要两倍的训练时间即可实现模型组合（类似取平均）的效果，非常高效。
VGG 简介 VGG是由牛津大学视觉几何小组（Visual Geometry Group, VGG）提出的一种深层卷积网络结构，赢得了 2014 年 ILSVRC 分类任务的亚军（冠军由 GoogLeNet 夺得）和定位任务的冠军。VGG可以看成是加深版本的AlexNet，都是conv layer + FC layer。该网络是在ILSVRC 2014上的相关工作，主要工作是证明了增加网络的深度能够在一定程度上影响网络最终的性能。
亮点 证明了增加网络的深度能够在一定程度上影响网络最终的性能。 小卷积核和小池化核，卷积核全部替换为3*3（极少用了1*1），相比AlexNet的3*3池化核，VGG全部为2*2的池化核。 层数更深特征图更宽，由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，计算量的增加放缓。 全连接层换成卷积层，网络测试阶段将训练阶段的三个全连接替换为三个卷积，测试重用训练时的参数，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入。 模型架构 网络特点 两个3x3的卷积堆叠获得的感受野大小，相当一个5x5的卷积；而3个3x3卷积的堆叠获取到的感受野相当于一个7x7的卷积。好处如下：
多层卷积引入了多次非线性变换，可以增加网络的非线性表达能力。 两层3*3卷积核的参数量比一层5*5卷积核的参数量更少，可以看作对网络做了相应的正则化。 网络测试阶段将训练阶段的三个全连接替换为三个卷积，使得测试阶段对输入图片尺寸没有限制。使用全连接层和卷积层的区别：
​	全连接层：通道数相同的情况下，特征图大小一样，才能保证与全连接层中的神经元数目相同。
​	卷积层：通道数目来决定最终的分类输出数目，不需要特征图大小相同。
GoogLeNet 简介 GoogLeNet是2014年Christian Szegedy提出的一种全新的深度学习结构，在这之前的AlexNet、VGG等结构都是通过增大网络的深度来获得更好的训练效果，但层数的增加会带来很多负作用，比如过拟合、梯度消失、梯度爆炸等。inception的提出则从另一种角度来提升训练结果：更高效的利用计算资源，在相同的计算量下能提取到更多的特征，从而提升训练结果。
前代网络缺陷 ​	LeNet，AlexNet，VGG这些串联网络通过增大网络的深度来获得更好的训练效果，主要存在以下缺陷：
提取的特征图尺度单一。串联网络中，每一层级的卷积核都是固定尺寸的，只能提取固定尺度的特征。基于这种尺度单一的特征图构建的模型鲁棒性不强，泛化能力差。 参数量巨大，且难以将梯度传递到网络顶层。虽然多层小卷积核堆叠取得大卷积核能够减少参数量，但杯水车薪，网络越深也越容易发生梯度消失，使得网络难以训练。 解决方案及亮点 使用1x1的卷积来进行升降维。1×1卷积核的重要作用：降维或升维、跨通道信息交融、减少参数量、增加模型深度并提高非线性表达能力。 与直觉相一致，即视觉信息应该在不同的尺度上进行处理再聚合。 引入稀疏特性、将全连接层换成稀疏连接，利用稀疏矩阵分解成密集矩阵计算的原理来加快收敛速度： 空间（spatial）维度，只对图像的某一部分进行卷积，而不是对整个图像进行卷积。 特征（feature）维度，在多个尺寸上进行卷积再聚合，把相关性强的特征聚集到一起，每一种尺寸的卷积只输出256个特征中的一部分。 各版本inception改进 inception-v1</description>
    </item>
    
    <item>
      <title>新一代视觉感知综述</title>
      <link>http://example.org/posts/%E6%96%B0%E4%B8%80%E4%BB%A3%E8%A7%86%E8%A7%89%E6%84%9F%E7%9F%A5%E7%BB%BC%E8%BF%B0/</link>
      <pubDate>Sat, 13 Aug 2022 23:47:44 +0800</pubDate>
      
      <guid>http://example.org/posts/%E6%96%B0%E4%B8%80%E4%BB%A3%E8%A7%86%E8%A7%89%E6%84%9F%E7%9F%A5%E7%BB%BC%E8%BF%B0/</guid>
      <description>经典篇 Visual Perception (视觉感知) Characterizing edges (特征边缘)：边缘是指图像强度函数中急剧变化的位置。
图像滤波：在尽量保留图像细节特征的条件下对目标图像的噪声进行抑制，是图像处理中不可缺少的操作，其处理效果的好坏将直接影响到后续图像处理和分析的有效性和可靠性。
canny边缘检测算法：
用高斯滤波器对图像进行平滑处理； 利用一阶偏导算子找到图像灰度沿水平方向和垂直方向的偏导数，并求出梯度的幅值和方位； 非极大值抑制，找到局部梯度最大值； 用双阈值算法检测和链接边缘，凡大于高阈值T1的一定是边缘，凡小于低阈值T2的一定不是边缘，若既大于低阈值又小于高阈值，则要看这个像素的邻接像素中是否有大于高阈值的边缘像素，若有则是边缘，否则不是。 Feature Extraction (特征提取) Bag of Words, BoW (词袋模型) 现代篇 CNN (卷积神经网络) Vision Transformer (视觉自注意力网络) 前沿篇 XAI (可解释性AI) Contrastive Pretraining (对比预训练） Multimodal Fusion (多模态融合） </description>
    </item>
    
    <item>
      <title>My_first_blog</title>
      <link>http://example.org/posts/my_first_blog/</link>
      <pubDate>Sat, 13 Aug 2022 23:04:49 +0800</pubDate>
      
      <guid>http://example.org/posts/my_first_blog/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
