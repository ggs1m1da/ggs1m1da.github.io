<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

  	<meta property="og:title" content=" Hzb&#39;s Study Blog" />
  	<meta property="og:site_name" content="Hzb&#39;s Study Blog" />
  	<meta property="og:url" content="http://example.org/" />
    
    
    <meta property="og:type" content="website" />
    

  <title>
     Hzb&#39;s Study Blog
  </title>

    <meta name="description" content="this is my description" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="http://example.org/images/favicon.ico">
	  <link rel="apple-touch-icon" href="http://example.org/images/apple-touch-icon.png" />
    
    <link rel="stylesheet" type="text/css" href="http://example.org/css/screen.css" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400|Inconsolata" />


    
      
      
        <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Hzb&#39;s Study Blog" />
      
    
    <meta name="generator" content="Hugo 0.107.0">

    <link rel="canonical" href="http://example.org/" />

     
</head>
<body class="nav-closed">
<div id="particles-js"></div>
  


 <div class="site-wrapper">



<header class="main-header " style="background-image: url(http://example.org/images/user.jpg)">


    

    <nav class="main-nav overlay clearfix">
        
            <a class="blog-logo" href="http://example.org/"><img src="http://example.org/images/user.png" alt="Blog Logo" /></a>
        
        
    </nav>
<div class="vertical">
        <div class="main-header-content inner">
            <h1 class="page-title">
              <a class="btn-bootstrap-2 title-scroll" href="#content">Hzb&#39;s Study Blog</a>
          </h1>
          <h2 class="page-description">this is my description</h2>
        </div>
</div>
    <a class="scroll-down icon-arrow-left" href="#content"><span class="hidden">Scroll Down</span></a>
</header>

<main id="content" class="content" role="main">
    
    
    
    
    
    

    <div class="extra-pagination inner">
        <nav class="pagination" role="navigation">
	
	<span class="page-number">Page 1 of 2</span>
	
	    <a class="older-posts" href="/page/2/">Older Posts &rarr;</a>
	
</nav>

    </div>

    
        
<article class="post posts">
    <header class="post-header">
        <h2 class="post-title"><a href="/posts/pspnet-deeplab/">PSPNet</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
          <section class="post-content">
            
          </section>
      </p>
      
    </section>
    <footer class="post-meta">
        
            <img class="author-thumb" src="http://example.org/images/user.png" alt="Author image" nopin="nopin" />
        
        
            syui
        
        
        <time class="post-date" datetime="2022-10-28T14:52:12&#43;08:00">
            28 Oct 2022
        </time>
    </footer>
</article>

    
        
<article class="post posts">
    <header class="post-header">
        <h2 class="post-title"><a href="/posts/fcn-u-net/">FCN &amp; U-Net</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
          <section class="post-content">
            <h2 id="fcn">FCN</h2>
<ul>
<li>
<h4 id="简介">简介</h4>
<p><a href="https://arxiv.org/abs/1411.4038">FCN</a>（Fully Convolutional Networks，全卷积网络）是Jonathan Long等人于2015年在Fully Convolutional Networks for Semantic Segmentation一文中提出的用于图像语义分割的一种框架，是深度学习用于语义分割领域的开山之作。FCN将传统CNN后面的全连接层换成了卷积层，这样网络的输出将是热力图而非类别；同时，为解决卷积和池化导致图像尺寸的变小，使用上采样方式对图像尺寸进行恢复。</p>
</li>
<li>
<h4 id="核心思想">核心思想</h4>
<ul>
<li>不含全连接层的全卷积网络，可适应任意尺寸输入；</li>
<li>反卷积层增大图像尺寸，输出精细结果；</li>
<li>结合不同深度层结果的跳级结构，确保鲁棒性和精确性。</li>
</ul>
<img src="/images/FCN1.jpg" alt="FCN1" style="zoom:80%;" />
</li>
<li>
<h4 id="网络结构">网络结构</h4>
<p>FCN是一个端到端，像素对像素的全卷积网络，用于进行图像的语义分割。整体的网络结构分为两个部分：全卷积部分和上采样部分。FCN中，使用了vgg16的卷积部分作为backbone，并将vgg16的最后三个全连接层也改为卷积层。除此之外，还增加了上采样部分，这里是使用转置卷积进行上采样.</p>
<img src="/images/FCN2.jpg" alt="FCN2" style="zoom:80%;" />
</li>
<li>
<h4 id="优缺点分析">优缺点分析</h4>
<ul>
<li>
<p>优点
FCN网络可以实现端到端的预测，可以接受任意大小的输入图像尺寸（因为没有全连接层），比较高效。</p>
</li>
<li>
<p>局限性
得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊的，对图像中的细节不敏感。而且在对各个像素进行分类时，没有考虑像素与像素之间的关系。</p>
</li>
</ul>
</li>
</ul>
<h2 id="u-net">U-Net</h2>
<ul>
<li>
<h4 id="简介-1">简介</h4>
<p>U-Net最早发表在2015的MICCAI上。后成为大多做医疗影像语义分割任务的baseline，也启发了大量研究者去思考U型语义分割网络。在自然影像理解方面，越来越多的语义分割和目标检测SOTA模型开始关注和使用U型结构，比如语义分割Discriminative Feature Network(DFN)(CVPR2018)，目标检测Feature Pyramid Networks for Object Detection(FPN)(CVPR 2017)等。</p>
</li>
<li>
<h4 id="网络结构-1">网络结构</h4>
<img src="/images/unet1.jpg" alt="unet1"  />
<p>UNet 网络结构如上图所示，其网络结构是对称的，形似英文字母 U，故而被称为 UNet 。就整体而言，UNet 是一个Encoder-Decoder的结构（与 FCN 相同），前半部分是特征提取，后半部分是上采样。</p>
<ul>
<li>Encoder：左半部分，由两个 3x3 的卷积层（ReLU）+ 一个 2x2 的 maxpooling 层组成一个下采样模块。由卷积操作和下采样操作组成，所用卷积结构统一为 3x3 的卷积核，padding=0，striding=1。没有 padding 所以每次卷积之后特征图的 H 和 W 变小了，在跳层连接（Skip connection）时需注意特征图的维度。</li>
<li>Decoder：右半部分，由一个上采样的卷积层 + 特征拼接 concat + 两个 3x3 的卷积层（ReLU）构成一个上采样模块。Decoder 用以恢复特征图的原始分辨率，除了卷积以外，该过程的关键步骤就是上采样与跳层连接。上采样常用转置卷积和插值两种方式实现。在插值实现方式中，双线性插值（bilinear）的综合表现较好也较为常见 。UNet 中的跳层连接通过拼接将底层的位置信息与深层的语义信息相融合。而在在 FCN 中，特征图是以相加的方式进行融合。</li>
</ul>
</li>
<li>
<h4 id="与fcn对比">与FCN对比</h4>
<ul>
<li>FCN：通过特征图对应像素值的相加来融合特征。特征图维度没有变化，但每个维度包含了更多特征。对于普通分类任务这种不需要从特征图复原到原始分辨率的任务来说，这是一个高效的选择；</li>
<li>UNet ：通过通道数的拼接，以形成更厚的特征（当然这样会更佳消耗显存）。拼接方式：保留了更多的维度和位置信息，这使得后面的网络层可在浅层特征与深层特征间自由选择，这对语义分割任务来说更具优势。</li>
</ul>
</li>
<li>
<h4 id="与fpn的对比">与FPN的对比</h4>
<ul>
<li>同：
<ol>
<li>都使用了“由上至下”、“横向连接”及“由下至上”的结构，从而对多尺度特征图进行融合，即将高层的语义信息与低层的几何细节结合。另外，融合后都会再经过一层卷积。</li>
</ol>
</li>
<li>异：
<ol>
<li>FPN对多尺度特征图融合的方式是element-wise add，而UNet采用的是concate；</li>
<li>FPN对多尺度特征图都进行了预测，而UNet仅在（由上至下）最后一层进行预测，而且这一层通常还需要进行一次resize才能恢复到原图尺寸；</li>
<li>FPN对高层特征图采用的放大方式是插值，而UNet通常还会使用转置卷积，通过网络自学习的方式来进行上采样；</li>
<li>FPN的高层特征放大2倍后与低层的尺寸恰好一致，而在UNet中通常不一致，还需要对低层特征做crop使得与放大后的高层特征尺寸一致；</li>
<li>FPN在下采样时的卷积带有padding，分辨率的下降仅由stirde决定，而UNet的卷积通常不带padding，使得分辨率下降在stride的基础上还会额外的减小。也就是说，FPN的“由下至上”和“由下至上”是对称结构，而UNet其实是非对称的，这也是导致4和2中最后提到的原因‘；</li>
<li>FPN在特征层融合后经过一层卷积是为了消除上采样过程中产生的混叠效应带来的影响，而UNet中还起到了压缩通道的作用（也是由于UNet融合特征层时采用的是concate，因此需要压缩通道减少计算量）；</li>
<li>FPN主要针对detection任务，而UNet针对segmentation任务，前者通常作为一个模块嵌入到网络结构中，而后者本身就是一种网络模型结构。</li>
</ol>
</li>
</ul>
</li>
<li>
<h4 id="为什么适用于医学图像">为什么适用于医学图像？</h4>
<ul>
<li>因为医学图像边界模糊、梯度复杂，需要较多的高分辨率信息。高分辨率用于精准分割。</li>
<li>人体内部结构相对固定，分割目标在人体图像中的分布很具有规律，语义简单明确，低分辨率信息能够提供这一信息，用于目标物体的识别。</li>
<li>UNet结合了低分辨率信息（提供物体类别识别依据）和高分辨率信息（提供精准分割定位依据），完美适用于医学图像分割。</li>
</ul>
</li>
</ul>
<h2 id="citation">Citation</h2>
<blockquote>
<p>北信科视觉感知研讨课程（周羿旭老师、赵永瑞同学分享）</p>
<p><a href="https://blog.csdn.net/m0_56192771/article/details/124113078">FCN网络解析</a></p>
<p><a href="https://blog.csdn.net/qq_41731861/article/details/120511148">FCN（全卷积神经网络）详解</a></p>
<p><a href="https://blog.csdn.net/qq_41731861/article/details/120528269">UNet 浅析</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/44958351">研习U-Net</a></p>
<p><a href="https://blog.csdn.net/m0_61899108/article/details/126734117">FPN与Unet的异同</a></p>
</blockquote>
<h1 id="thanks-for-reading"><em>Thanks for reading!</em></h1>

          </section>
      </p>
      
    </section>
    <footer class="post-meta">
        
            <img class="author-thumb" src="http://example.org/images/user.png" alt="Author image" nopin="nopin" />
        
        
            syui
        
        
        <time class="post-date" datetime="2022-10-27T19:16:59&#43;08:00">
            27 Oct 2022
        </time>
    </footer>
</article>

    
        
<article class="post posts">
    <header class="post-header">
        <h2 class="post-title"><a href="/posts/retinanet-fcos/">RetinaNet &amp; FCOS</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
          <section class="post-content">
            <h2 id="retinanet">RetinaNet</h2>
<ul>
<li>
<h4 id="简介">简介</h4>
<p><a href="https://arxiv.org/pdf/1708.02002.pdf">RetinaNet</a> 是 Tsung-Yi Lin 和 Kaiming He（四作） 于 2018 年发表的论文 Focal Loss for Dense Object Detection。深入分析了极度不平衡的正负（前景背景）样本比例导致 one-stage 检测器精度低于 two-stage 检测器，基于上述分析，提出了一种简单但是非常实用的 Focal Loss 焦点损失函数，并且 Loss 设计思想可以推广到其他领域，同时针对目标检测领域特定问题，设计了 RetinaNet 网络，结合 Focal Loss 使得 one-stage 检测器在精度上能够达到乃至超过 two-stage 检测器。</p>
</li>
<li>
<h4 id="网络结构">网络结构</h4>
<p><img src="/images/retinanet1.jpg" alt="retinanet1"></p>
<p>RetinaNet的特征提取网络选择了残差网络ResNet，特征融合这块选择了FPN（特征金字塔网络），以特征金字塔不同的尺寸特征图作为输入，搭建三个用于分类和框回归的子网络。分类网络输出的特征图尺寸为（W,H,KA)，其中W、H为特征图宽高，KA为特征图通道，存放A个anchor各自的类别信息（K为类别数）。</p>
</li>
<li>
<h4 id="历史问题">历史问题</h4>
<p><img src="/images/retinanet2.jpg" alt="retinanet2"></p>
<ul>
<li>
<p>在One stage中，detector直接在类别不平衡（负样本很多，正样本很少）中进行分类和回归，直接输出bbox和类别，原有的交叉熵损失无法处理这种不平衡，导致训练不充分，精度低，但是却提升了检测速度。</p>
</li>
<li>
<p>在Two stage中，FPN网络已经过滤了一部分的背景bbox，因此在fast r-cnn中正负样本比例较均衡，因此准确率较高。</p>
</li>
<li>
<p>针对所有的负样本，数量过多，主导了损失函数，不利于模型收敛。</p>
</li>
<li>
<p>针对单个负样本，大多数负样本不包含任何物体，属于易分样本，且易分样本数量很多，训练时对应背景类的预测得分会很高，那么单个样本的loss就很小，反向计算时梯度小，造成易分负样本对loss的收敛作用有限。</p>
</li>
</ul>
</li>
<li>
<h4 id="主要贡献">主要贡献</h4>
<p>提出<strong>Focal Loss</strong>：解决one-stage算法中，样本不平衡和难易样本的问题。</p>
<ul>
<li>
<p>样本不平衡：保证在损失函数中，正样本与负样本的贡献（比重）均衡。</p>
</li>
<li>
<p>难易样本：希望模型更关注难分样本，难分样本在loss中的比重更大。</p>
</li>
</ul>
</li>
<li>
<h4 id="focal-loss">Focal Loss</h4>
<ul>
<li>
<p>二分类交叉熵损失函数：</p>
<p><img src="/images/retinanet3.jpg" alt="retinanet3"></p>
</li>
<li>
<p>解决正负样本不均：加上一个权重α（范围[0,1]）</p>
<p><img src="/images/retinanet5.jpg" alt="retinanet5"></p>
</li>
<li>
<p>解决难易样本不均：引入因子γ(≥0)，将高置信度的样本的损失权重降低</p>
<p><img src="/images/retinanet8.png" alt="retinanet8"></p>
</li>
<li>
<p>Focal Loss</p>
<p><img src="/images/retinanet6.jpg" alt="retinanet6"></p>
<p>展开后为</p>
<p><img src="/images/retinanet7.jpg" alt="retinanet7"></p>
</li>
</ul>
</li>
<li>
<h4 id="优缺点分析">优缺点分析</h4>
<ul>
<li>
<p>优点：提升准确率，降低了正负样本和难易样本不均衡带来的影响；</p>
</li>
<li>
<p>缺点：Focal Loss易受噪声干扰，对图像标注的准确性要求非常高，一旦有标错的样本，就会被focal loss当做困难样本，从而影响学习效果。</p>
</li>
</ul>
</li>
</ul>
<h2 id="fcos">FCOS</h2>
<ul>
<li>
<h4 id="简介-1">简介</h4>
<p><a href="https://arxiv.org/pdf/2006.09214.pdf">FCOS</a> 发表于ICCV 2019，是一阶段anchor free目标检测算法，其主要的卖点为无锚。通过回归特征图上每个位置距离目标框的上下左右距离来实现目标检测。如果一个位置落在了多个目标框内，文中的方法是通过多尺度+回归幅度限制的方法来缓解这个问题。为了解决目标框数量过多的问题，文中提出了center-ness的方法，为每个位置学习一个center-ness分数，最后乘以预测类别分数作为非极大抑制的输入参数来解决这个问题。</p>
</li>
<li>
<h4 id="核心思想">核心思想</h4>
<p>将铺设锚框变为铺设锚点，进行物体检测。</p>
<p>所谓铺设锚框，又称为Anchor-Based，是指在输出特征图上的每个像素的位置，放置几个预先定义的anchor框，在网络训练过程中，对这些anchor框进行分类与回归。
通过GT框和这些anchor框计算IoU，依据设定的阈值条件来定义正负样本，典型的有YOLOv3。</p>
<p>所谓铺设锚点，又称为Anchor-free，如下图所示，将原有的 对锚框进行分类与回归，变为对锚点进行分类与回归，其中回归是预测锚点到检测（GT）框上下左右四条边界的距离，典型的有FCOS。</p>
</li>
<li>
<h4 id="anchor-based缺点">Anchor-Based缺点</h4>
<ul>
<li>Anchor-Based方式，检测性能对于anchor的大小、数量、长宽比 都非常敏感，通过改变这些超参数Retinanet在COCO benchmark上面提升了4%的MAP。</li>
<li>固定size和aspect ratio的anchor损害了检测器的普适性，导致对于不同任务，anchor可能需要重新设置大小和长宽比。</li>
<li>为了达到更高的召回率（查全率），需要生成大量的anchor（FPN约18万个），但是大部分的anchor在训练时标记为负样本（negative），造成了样本不均衡问题。</li>
<li>在训练中，需要计算所有anchor与真实框的IoU，这样会消耗大量内存和计算资源。</li>
</ul>
</li>
<li>
<h4 id="fcos优点">FCOS优点</h4>
<ul>
<li>检测问题被统一到 FCN-solvable 的问题，可以简单地重用其他任务的idea，如语义分割。</li>
<li>anchor-free方式，不需要像anchor-based那样大量调整参数，使训练更为简单。</li>
<li>由于不需要计算IoU，节省了大量算力和内存。</li>
<li>提出了一些关于交叠区域的解决方法和思考。</li>
<li>模型部署会受到两种限制，一种是计算量的限制，一种是I/O 带宽的限制。anchor-free方式相比于anchor-based方式，对部署更友好一些。</li>
</ul>
</li>
<li>
<h4 id="网络结构-1">网络结构</h4>
<img src="/images/fcos1.jpg" alt="fcos1" style="zoom:67%;" />
<p>FCOS采用FPN结构，backbone的C3、C4、C5特征层作为FPN的输入，FPN生成P3、P4、P5、P6、P7特征图，送入后续的检测头Head。</p>
<p>每个Head包含3个分支：</p>
<ul>
<li>classification分支：预测类别，图中的C表示类别数，相当于C个二分类；</li>
<li>regression分支：回归位置，图中的4表示：l、t、r、b，预测锚点到检测框上下左右四条边界的距离；</li>
<li>center-ness：中心度，一个锚点对应一个中心度，用于锚点相对于检测框中心性的判断
在检测子网络Head中，分类分支和回归分支都先经过了4个卷积层进行了特征强化。</li>
</ul>
<p>早期版本，在分类分支中，既包含 正、负样本锚点的 类别预测分支，又包含正、负样本锚点中心性判断的center-ness分支，用来强化检测结果；</p>
<p>回归分支用来回归正样本锚点到检测框上、下、左、右四个边界的距离 。
<img src="/images/fcos2.jpg" alt="fcos2" style="zoom: 80%;" /></p>
</li>
<li>
<h4 id="损失函数">损失函数</h4>
<img src="/images/fcos3.jpg" alt="fcos3" style="zoom:67%;" />
</li>
</ul>
<h2 id="citation">Citation</h2>
<blockquote>
<p>北信科视觉感知研讨课程（高丹阳师姐分享）</p>
<p><a href="https://www.jianshu.com/p/4dbf876d1fae">RetinaNet</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/410436667">一阶段目标检测器-RetinaNet网络详解</a></p>
<p><a href="https://blog.csdn.net/zl_Dream/article/details/123730886">目标检测FCOS的初步理解</a></p>
<p><a href="https://blog.csdn.net/weixin_45377629/article/details/124844405">FCOS理论知识讲解</a></p>
</blockquote>
<h1 id="thanks-for-reading"><em>Thanks for reading!</em></h1>

          </section>
      </p>
      
    </section>
    <footer class="post-meta">
        
            <img class="author-thumb" src="http://example.org/images/user.png" alt="Author image" nopin="nopin" />
        
        
            syui
        
        
        <time class="post-date" datetime="2022-10-27T10:13:28&#43;08:00">
            27 Oct 2022
        </time>
    </footer>
</article>

    
        
<article class="post posts">
    <header class="post-header">
        <h2 class="post-title"><a href="/posts/fpn-ssd-yolo/">FPN &amp; SSD &amp; YOLO</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
          <section class="post-content">
            <h2 id="fpn">FPN</h2>
<h4 id="提出原因">提出原因</h4>
<p>卷积网络中，深层网络容易响应语义特征，浅层网络容易响应图像特征。然而，在目标检测中往往因为卷积网络的这个特征带来了不少麻烦：高层网络虽然能响应语义特征，但是由于Feature Map的尺寸太小，拥有的几何信息并不多，不利于目标的检测；浅层网络虽然包含比较多的几何信息，但是图像的语义特征并不多，不利于图像的分类。这个问题在小目标检测中更为突出。因此需要能够合并深层和浅层特征的网络，同时满足目标检测和图像分类的需要。</p>
<h4 id="参考思想">参考思想</h4>
<p><a href="https://arxiv.org/abs/1612.03144">FPN</a>使用的是图像金字塔的思想。</p>
<p>传统的图像金字塔采用输入多尺度图像的方式构建多尺度的特征。输入一张图像后，可以通过一些手段获得多张不同尺度的图像，将这些不同尺度图像的4个顶点连接起来，就可以构造出一个类似真实金字塔的一个图像金字塔。整个过程有点像是我们看一个物品由远及近的过程（近大远小原理）。</p>
<p>其中，中间的图像是原始图像，尺寸越来越小的图片是经过下采样处理后的结果，而尺寸越来越大的图片是经过上采样处理后的结果。把高层的特征传下来，补充低层的语义，这样就可以获得高分辨率、强语义的特征，有利于小目标的检测。</p>
<h4 id="特征金字塔">特征金字塔</h4>
<p>运用金字塔的思想可以提高算法的性能，但是需要大量的运算和内存。因此特征金字塔要在速度和准确率之间进行权衡，通过它获得更加鲁棒的语义信息。</p>
<p>图像中存在不同大小的目标，而不同的目标具有不同的特征，所以需要特征金字塔来利用浅层的特征将简单的目标区分开，利用深层的特征将复杂的目标区分开。即利用大的特征图区分简单目标，利用小的特征图区分复杂目标。</p>
<h4 id="具体思路">具体思路</h4>
<img src="/images/fpn1.png" alt="fpn1" style="zoom: 50%;" />
<ul>
<li>
<p>图（a）：</p>
<p>先对原始图像构造图像金字塔，然后在图像金字塔的每一层提出不同的特征，然后进行相应的预测。</p>
<p>优点：精度较好。</p>
<p>缺点：计算量和占用内存太大。</p>
</li>
<li>
<p>图（b）：</p>
<p>通过对原始图像进行卷积和池化操作来获得不同尺寸的feature map，在图像的特征空间中构造出金字塔。因为浅层的网络更关注于细节信息，高层的网络更关注于语义信息，更有利于准确检测出目标，因此利用最后一个卷积层上的feature map来进行预测分类。</p>
<p>优点：速度快、内存少。</p>
<p>缺点：仅关注深层网络中最后一层的特征，却忽略了其它层的特征。</p>
</li>
<li>
<p>图（c）：</p>
<p>同时利用低层特征和高层特征。就是首先在原始图像上面进行深度卷积，然后分别在不同的特征层上面进行预测。</p>
<p>优点：在不同的层上面输出对应的目标，不需要经过所有的层才输出对应的目标（即对于有些目标来说，不用进行多余的前向操作），速度更快，又提高了算法的检测性能。</p>
<p>缺点：获得的特征不鲁棒，都是一些弱特征（因为很多的特征都是从较浅的层获得的）。</p>
</li>
<li>
<p>图（d）：</p>
<p>FPN网络，对最底层的特征进行向上采样，并与该底层特征进行融合，得到高分辨率、强语义的特征（即加强了特征的提取）。</p>
</li>
</ul>
<h4 id="整体过程">整体过程</h4>
<img src="/images/fpn2.jpg" alt="fpn2" style="zoom: 80%;" />
<ol>
<li>自下而上：先把预处理好的图片送进预训练的网络，如ResNet，构建自下而上的网络，对应上图左侧金字塔。</li>
<li>自上而下：左侧顶层直接复制到右侧顶层，对右侧顶层进行上采样操作（就是2 * up），再用1 * 1卷积对左侧次顶层进行降维处理，然后将两者对应元素相加（这里就是高低层特征的一个汇总），后续以此类推，如此构成自上而下网络。</li>
<li>卷积融合：最后我们对右侧各层分别来一个3 * 3卷积操作得到最终的预测（对应上图的predict）。</li>
</ol>
<h2 id="ssd">SSD</h2>
<h4 id="简介">简介</h4>
<p><a href="https://arxiv.org/pdf/1512.02325.pdf">SSD</a>，全称Single Shot MultiBox Detector，是Wei Liu在ECCV 2016上提出的一种目标检测算法，截至目前是主要的检测框架之一，相比Faster RCNN有明显的速度优势，相比YOLO又有明显的mAP优势。</p>
<h4 id="背景">背景</h4>
<p>目标检测主流算法分成两个类型：</p>
<ol>
<li>
<p>two-stage方法：RCNN系列</p>
<p>通过算法产生候选框，然后再对这些候选框进行分类和回归。</p>
</li>
<li>
<p>one-stage方法：yolo和SSD</p>
<p>直接通过主干网络给出类别位置信息，不需要区域生成。</p>
</li>
</ol>
<h4 id="特点">特点</h4>
<ol>
<li>从YOLO中继承了将detection转化为regression的思路，一次完成目标定位与分类。</li>
<li>基于Faster RCNN中的Anchor，提出了相似的prior box。</li>
<li>加入基于特征金字塔（Pyramidal Feature Hierarchy）的检测方式，即在不同感受野的feature map上预测目标。</li>
<li>这些设计实现了简单的端到端的训练，而且即便使用低分辨率的输入图像也能得到高的精度。</li>
</ol>
<h4 id="网络结构">网络结构</h4>
<ol>
<li>
<p>采用多尺度特征图用于检测</p>
<p>CNN网络一般前面的特征图比较大，后面会逐渐采用stride=2的卷积或者pool来降低特征图大小，这正如图3所示，一个比较大的特征图和一个比较小的特征图，它们都用来做检测。这样做的好处是比较大的特征图来用来检测相对较小的目标，而小的特征图负责检测大目标。</p>
</li>
<li>
<p>采用卷积进行检测</p>
<p>SSD直接采用卷积对不同的特征图来进行提取检测结果。对于形状为m<em>n</em>p的特征图，只需要采用3<em>3</em>p这样比较小的卷积核得到检测值。（每个添加的特征层使用一系列卷积滤波器可以产生一系列固定的预测）。</p>
</li>
<li>
<p>设置先验框
SSD借鉴faster rcnn中ancho理念，每个单元设置尺度或者长宽比不同的先验框，预测的是对于该单元格先验框的偏移量，以及每个类被预测反映框中该物体类别的置信度。</p>
</li>
</ol>
<h4 id="模型结构">模型结构</h4>
<p>SSD的模型框架主要由三部分组成，以SSD300为例，有VGG-Base Extra-Layers，Pred-Layers。</p>
<p>VGG-Base作为基础框架用来提取图像的feature，Extra-Layers对VGG的feature做进一步处理，增加模型对图像的感受野，使得extra-layers得到的特征图承载更多抽象信息。待预测的特征图由六种特征图组成，6中特征图最终通过pred-layer得到预测框的坐标，置信度，类别信息。</p>
<img src="/images/ssd1.png" alt="ssd1" style="zoom: 67%;" />
<h2 id="yolo">YOLO</h2>
<h4 id="简介-1">简介</h4>
<p>YOLO在2016年被提出，发表于CVPR。YOLO的全称是you only look once，指只需要浏览一次就可以识别出图中的物体的类别和位置。因为只需要看一次，YOLO被称为Region-free方法，相比于Region-based方法，YOLO不需要提前找到可能存在目标的Region。</p>
<h4 id="具体思路-1">具体思路</h4>
<p>将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。每个bounding box要预测(x, y, w, h)和confidence共5个值，每个网格还要预测一个类别信息，记为C类。则SxS个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是S x S x (5*B+C)的一个tensor，网络主干是GooleNet。</p>
<h4 id="存在问题">存在问题</h4>
<ol>
<li>损失函数中localization error和classification error同等重要（解决办法可以是：对没有object的box的confidence loss，赋予小的loss weight；只有当某个网格中有object的时候才对classification error进行更新）。</li>
<li>输出为全连接层，只支持与训练图像相同的输入分辨率。</li>
<li>YOLO对相互靠的很近的物体（挨在一起且中点都落在同一个格子上的情况），还有对很小的物体检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类。虽然每个格子可以预测B个bounding box，但是最终只选择只选择IOU最高的bounding box作为物体检测输出，即每个格子最多只预测出一个物体。当物体占画面比例较小，如图像中包含畜群或鸟群时，每个格子包含多个物体，但却只能检测出其中一个。这是YOLO方法的一个缺陷。</li>
<li>测试图像中，当同一类物体出现的不常见的长宽比和其他情况时泛化能力偏弱。</li>
<li>对不同大小的box预测中，相比于大box预测偏一点，小box预测偏一点肯定更不能被忍受的。而sum-square error loss中对同样的偏移loss是一样(为了缓和这个问题，作者用了一个比较取巧的办法，就是将box的width和height取平方根代替原本的height和width)。定位误差是影响检测效果的主要原因，尤其是大小物体的处理上，还有待加强。</li>
<li>YOLO loss函数中，大物体IOU误差和小物体IOU误差对网络训练中loss贡献值接近（虽然采用求平方根方式，但没有根本解决问题）。因此，对于小物体，小的IOU误差也会对网络优化过程造成很大的影响，从而降低了物体检测的定位准确性。</li>
</ol>
<h2 id="citation">Citation</h2>
<blockquote>
<p>北信科视觉感知研讨课程（周羿旭老师、赵永瑞同学分享）</p>
<p><a href="https://blog.csdn.net/weixin_55073640/article/details/122627966">深度学习中的FPN详解</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/397293649">什么是FPN(Feature Pyramid Networks&ndash;特征金字塔)？</a></p>
<p><a href="https://blog.csdn.net/toCVer/article/details/125445322">SSD网络介绍</a></p>
<p><a href="https://blog.csdn.net/jiugeshao/article/details/124362788">YOLO系列知识点整理</a></p>
</blockquote>
<h1 id="thanks-for-reading"><em>Thanks for reading!</em></h1>

          </section>
      </p>
      
    </section>
    <footer class="post-meta">
        
            <img class="author-thumb" src="http://example.org/images/user.png" alt="Author image" nopin="nopin" />
        
        
            syui
        
        
        <time class="post-date" datetime="2022-10-26T15:42:33&#43;08:00">
            26 Oct 2022
        </time>
    </footer>
</article>

    
        
<article class="post posts">
    <header class="post-header">
        <h2 class="post-title"><a href="/posts/rcnn-fast-rcnn-faster-rcnn/">Rcnn &amp; Fast-Rcnn &amp; Faster-Rcnn</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
          <section class="post-content">
            <h2 id="rcnn">RCNN</h2>
<ul>
<li>
<h4 id="简介">简介</h4>
<p><a href="https://arxiv.org/pdf/1311.2524.pdf">R-CNN</a>的全称是Region-CNN，是第一个成功将深度学习应用到目标检测上的算法。R-CNN基于卷积神经网络，线性回归，和支持向量机等算法，实现目标检测技术。R-CNN遵循传统目标检测的思路，同样采用提取框，对每个框提取特征、图像分类、 非极大值抑制四个步骤进行目标检测。只不过在提取特征这一步，将传统的特征（如 SIFT、HOG 特征等）换成了深度卷积网络提取的特征。</p>
</li>
<li>
<h4 id="算法流程">算法流程</h4>
<img src="/images/rcnn1.png" alt="rcnn1" style="zoom:67%;" />
<ul>
<li>
<p>候选区域生成</p>
<p>使用Selective Search算法对每张输入图像使用选择性搜索来选取多个高质量的候选区域（Region Proposal）。这个算法先对图像基于像素信息做快速分割来得到多个区域，然后将当下最相似的两区域合并成一个区域，重复进行合并直到整张图像变成一个区域。最后根据合并的信息生成多个有层次结构的提议区 域，并为每个提议区域生成物体类别和真实边界框。</p>
</li>
<li>
<p>特征提取</p>
<p>选取一个预先训练好的卷积神经网络，去掉最后的输出层来作为特征抽取模块。对每个提议区域，将其变形成卷积神经网络需要的输入尺寸后进行前向计算抽取特征。</p>
</li>
<li>
<p>SVM分类器</p>
<p>将每个提议区域的特征连同其标注做成一个样本，训练多个支持向量机(SVM)来进行物 体类别分类，这里第 i 个 SVM 预测样本是否属于第 i 类。</p>
</li>
<li>
<p>边界框回归器</p>
<p>在这些样本上训练一个线性回归模型来预测（精修）真实边界框。对于SVM分好类的候选区域做边框回归，用Bounding box回归值校正原来的建议窗口，生成预测窗口坐标</p>
</li>
</ul>
</li>
<li>
<h4 id="selective-search">Selective Search</h4>
<p>Selective Search（选择性搜索）是用于目标检测的region proposal算法，它计算速度快，具有很高的召回率，基于颜色，纹理，大小和形状兼容计算相似区域的分层分组。</p>
<p>图像中区域特征比像素更具代表性，作者使用Felzenszwalb and Huttenlocher的方法产生图像初始区域，使用贪心算法对区域进行迭代分组：</p>
<ol>
<li>计算所有邻近区域之间的相似性；</li>
<li>两个最相似的区域被组合在一起；</li>
<li>计算合并区域和相邻区域的相似度；</li>
<li>重复2、3过程，直到整个图像变为一个地区。</li>
</ol>
<p>在每次迭代中，形成更大的区域并将其添加到区域提议列表中。以自下而上的方式创建从较小的细分segments到较大细分segments的region proposal。</p>
</li>
<li>
<h4 id="bounding-box-regression">Bounding box regression</h4>
<p>Rcnn的Bounding box regression（边框回归）是指对粗略的预测框P进行平移和尺度放缩来微调， 使得经过微调后的窗口G&rsquo;跟Ground Truth G更接近， 这样定位会更准确，其详细数学原理参考<a href="https://blog.csdn.net/zijin0802034/article/details/77685438">边框回归(Bounding Box Regression)详解</a>。</p>
<h2 id="citation">Citation</h2>
<blockquote>
<p>北信科视觉感知研讨课程（赵永瑞同学分享）</p>
<p><a href="https://blog.csdn.net/zijin0802034/article/details/77685438">边框回归(Bounding Box Regression)详解</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/39927488">理解Selective Search</a></p>
</blockquote>
<h1 id="thanks-for-reading"><em>Thanks for reading!</em></h1>
</li>
</ul>

          </section>
      </p>
      
    </section>
    <footer class="post-meta">
        
            <img class="author-thumb" src="http://example.org/images/user.png" alt="Author image" nopin="nopin" />
        
        
            syui
        
        
        <time class="post-date" datetime="2022-09-21T16:55:23&#43;08:00">
            21 Sep 2022
        </time>
    </footer>
</article>

    
        
<article class="post posts">
    <header class="post-header">
        <h2 class="post-title"><a href="/posts/shufflenet-efficientnet/">ShuffleNet &amp; EfficientNet</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
          <section class="post-content">
            <h2 id="shufflenet">ShuffleNet</h2>
<ul>
<li>
<h4 id="简介">简介</h4>
<p><a href="https://arxiv.org/pdf/1707.01083.pdf">ShuffleNet</a>是旷视科技提出的一种计算高效的CNN模型，其和MobileNet和SqueezeNet等一样主要是想应用在移动端。所以，ShuffleNet的设计目标也是如何利用有限的计算资源来达到最好的模型精度，这需要很好地在速度和精度之间做平衡。ShuffleNet是一种专门为计算资源有限的设备设计的神经网络结构，主要采用了pointwise group convolution和channel shuffle两种技术，在保留了模型精度的同时极大减少了计算开销。</p>
</li>
<li>
<h4 id="旧网络存在问题">旧网络存在问题</h4>
<ul>
<li>分组卷积GConv虽然能减少参数量和计算量，但是GConv中不同的组之间的信息没有交流。</li>
<li>在ResNeXt网络中，分组卷积占的计算量很少，大部分计算量都是1*1卷积产生的。</li>
</ul>
</li>
<li>
<h4 id="创新点">创新点</h4>
<ul>
<li>
<p>Channel Shuffle</p>
<p><img src="/images/shufflenet1.png" alt="shufflenet1"></p>
<p>先通过一个分组卷积，得到对应的特征矩阵，接下来使用channel shuffle操作后的特征矩阵进行组卷积，融合不同组之间的维度信息。</p>
<p>具体为，假设输入的feature map是一个一维的12个数据，分为3组，每一个分组有4个值，channel shuffle操作首先将这个矩阵进行升维，重构为一个g行n列的矩阵（这里g=3，n=4），然后对这个矩阵进行转置，后得到一个n行g列的新矩阵，再按照分组数g对这个得到的新矩阵进行展开，通过这一系列操作，达到了将原来固定的各分组打乱重排的目的。</p>
</li>
<li>
<p>ShuffleNet Unit</p>
<p>ShuffleNet Unit的结构灵感来源于ResNet的中的bottleneck残差模块，从某种意义上来说，二者看起来几乎一样，以下是ShuffleNet Unit的结构：</p>
<p><img src="/images/shufflenet2.png" alt="shufflenet2"></p>
<p>相较于ResNet的bottleneck模块，ShuffleNet Unit有三处差别：</p>
<ol>
<li>ShuffleNet Unit将bottleneck模块中的3x3卷积替换为了3x3的depthwise卷积，也就是图中的DWConv，如图a所示。</li>
<li>对原残差模块的1x1卷积进行了替换，替换为了1x1的分组卷积，并在后面加上了一个channel shuffle操作，用于消除分组卷积的副作用，如图b所示。</li>
<li>在bottleneck的短路路径上使用了3x3的全局平均池化，然后将原来bottleneck模块的逐元素求和替换为了通道（Add）的拼接操作（Concat），如图c所示。使原来的数值1相加操作变成了维度的相加，更高的维度可以使网络拥有更多的通道数，从而提升网络的性能。</li>
</ol>
</li>
</ul>
</li>
<li>
<h4 id="网络结构">网络结构</h4>
<p>基于shufflenet unit模块，整体的ShuffleNet网络结构图如下图所示：</p>
<img src="/images/shufflenet3.jpg" alt="shufflenet3" style="zoom: 67%;" />
<p>上图为一个图像分类任务，输入为224x224的图像，输出1000个概率，同时作者将分组数分别设置为1到5，以此探索不同的分组数对网络性能的影响，其中一般以g=3作为网络的基准版本。</p>
<p>与传统的神经网络相比，ShuffleNet网络结构最大的区别在于拥有三个shufflenet unit模块，分别对应三个Stage，它们将输入特征图的尺寸减半，通道数加倍，除了升级版的shufflenet unit模块，网络的其他超参数与ResNet保持一致。</p>
</li>
</ul>
<h2 id="efficient-net">Efficient Net</h2>
<ul>
<li>
<h4 id="简介-1">简介</h4>
<p><a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNets</a>是google在2019年5月发表的一个网络系列，使用神经架构搜索设计了一个baseline网络，并且将模型进行缩放获得一系列模型。它的精度和效率比之前所有的卷积网络都好。尤其是EfficientNet-B7在ImageNet上获得了当时最先进的 84.4%的top-1精度 和 97.1%的top-5精度，同时比之前最好的卷积网络大小缩小了8.4倍、速度提高了6.1倍。EfficientNets也可以很好的迁移，并且以更少的参量实现了最先进的精度——CIFAR-100（91.7%）、Flowers（98.8%）。</p>
</li>
<li>
<h4 id="背景">背景</h4>
<p>通常为了获得更好的精度，放大卷积神经网络是一种广泛的方法。举个例子，ResNet可以通过使用更多层从ResNet-18放大到ResNet-200；目前为止，GPipe通过将baseline模型放大四倍在ImageNet数据集上获得了84.3%的top-1精度，然而，放大CNN的过程从来没有很好的理解过，目前通用的几种方法是放大CNN的深度、宽度和分辨率，在之前都是单独放大这三个维度中的一个，尽管任意放大两个或者三个维度也是可能的，但是任意缩放需要繁琐的人工调参同时可能产生的是一个次优的精度和效率。</p>
</li>
<li>
<h4 id="模型缩放方法">模型缩放方法</h4>
<ul>
<li>
<p>Single Scaling（单尺度）</p>
<ol>
<li>深度（d）：缩放网络深度在许多ConvNets都有使用，直觉上更深的网络可以捕获到更丰富和更复杂的特征，在新任务上也可以泛化的更好。然而，更深的网络由于梯度消失问题（这里我更倾向于说成是网络退化问题）也更难训练。尽管有一些技术，例如跨层连接、批量归一化等可以有效减缓训练问题，但是深层网络的精度回报减弱了：举个例子，ResNet-1000和ResNet-101具有类似的精度，即使它的层数更多。</li>
<li>宽度（w）：缩放网络宽度也是一种常用的手段，正如之前讨论过的，更宽的网络可以捕捉到更细粒度的特征从而易于训练。然而，非常宽而又很浅的网络在捕捉高层次特征时有困难。</li>
<li>分辨率（r）：使用更高分辨率的输入图像，ConvNets可能可以捕捉到更细粒度的模式。从最早的 224x224，现在有些ConvNets为了获得更高的精度选择使用 229x229 或者 331x331。目前，GPipe使用 480x480 的分辨率获得了最先进的ImageNet精度，更好的精度比如 600x600 也被广泛使用在目标检测网络中。</li>
</ol>
</li>
<li>
<p>Compound Scaling（混合尺度）</p>
<p>论文中提出了Compound Scaling，同时对通过NAS得到的baseline模型Efficient-b0的深度（depth）、宽度（width）、输入图片分辨率（resolution）进行scaling，和 AutoML技术得到一系列网络模型EfficientNets。</p>
</li>
<li>
<p>为了了解网络缩放的效果，作者系统地研究了缩放不同维数对模型的影响。 虽然缩放单个维度可以提高模型性能，但作者观察到，根据可用资源平衡网络的所有维度ーー宽度、深度和图像分辨率ーー可以最大限度地提高整体性能。</p>
</li>
<li>
<p>复合缩放方法的第一步是执行网格搜索，以找到在固定资源约束下基线网络的不同缩放维度之间的关系。这决定了上面提到的每个维度的适当比例系数。 然后应用这些系数扩大基线网络，以达到期望的模型大小或资源要求。</p>
</li>
</ul>
<img src="/images/efficientnet1.png" alt="efficientnet1" style="zoom: 67%;" />
<ul>
<li>与传统的缩放方法（上图a-d）相比，这种复合缩放方法（上图e）不断提高模型的精度和效率，可用于扩展现有的模型，如 mobileet (+ 1.4% 的图像集精度)和 ResNet (+ 0.7%)。</li>
</ul>
</li>
<li>
<h4 id="网络结构-1">网络结构</h4>
<p>作者指明，由于模型缩放不会改变基线网络中的层，但是拥有一个良好的基线网络也是至关重要的。作者使用现有的基础网络来评估该缩放方法，但是为了更好地证明该缩放方法的有效性，作者还开发了一种新的mobile-size baseline，称为 EfficientNet，EfficientNet-B0的网络结构如下 (类似于 MobileNetV2 和 MnasNet)：</p>
<img src="/images/efficientnet2.png" alt="efficientnet2" style="zoom: 67%;" />
</li>
</ul>
<h2 id="citation">Citation</h2>
<blockquote>
<p>北信科视觉感知研讨课程（周羿旭老师分享）</p>
<p><a href="https://zhuanlan.zhihu.com/p/482776750">【轻量化网络】ShuffleNet V1 论文研读</a></p>
<p><a href="https://blog.csdn.net/loki2018/article/details/124077822">论文阅读笔记：ShuffleNet</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/96773680">令人拍案叫绝的EfficientNet和EfficientDet</a></p>
<p><a href="https://blog.csdn.net/Q1u1NG/article/details/106317925">EfficientNet详解</a></p>
</blockquote>
<h1 id="thanks-for-reading"><em>Thanks for reading!</em></h1>

          </section>
      </p>
      
    </section>
    <footer class="post-meta">
        
            <img class="author-thumb" src="http://example.org/images/user.png" alt="Author image" nopin="nopin" />
        
        
            syui
        
        
        <time class="post-date" datetime="2022-09-12T15:05:52&#43;08:00">
            12 Sep 2022
        </time>
    </footer>
</article>

    
        
<article class="post posts">
    <header class="post-header">
        <h2 class="post-title"><a href="/posts/senet-mobilenet/">SENet &amp; MobileNet</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
          <section class="post-content">
            <h2 id="senet">SENet</h2>
<ul>
<li>
<h4 id="简介">简介</h4>
<p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SENet</a>是2017ILSVRC2017（ImageNet Large Scale Visual Recognition Challenge）竞赛的冠军网络，在CVPR2018引用量第一。在深度学习领域，已经有很多成果通过在空间维度上对网络的性能进行了提升。而SENet反其道而行之，通过对通道关系进行建模来提升网络的性能。SENet较早的将注意力机制引入CNN中，使用了模块化设计。</p>
</li>
<li>
<h4 id="亮点">亮点</h4>
<p>引入通道注意力机制，关注channel之间的关系，希望模型可以自动学习到不同channel特征的重要程度。</p>
</li>
<li>
<h4 id="网络结构">网络结构</h4>
<img src="/images/senet1.jpg" alt="senet1" style="zoom: 67%;" />
</li>
<li>
<h4 id="原理">原理</h4>
<p>SENet网络的创新点在于关注channel之间的关系，希望模型可以自动学习到不同channel特征的重要程度。为此，SENet提出了Squeeze-and-Excitation (SE)模块，SE模块首先对卷积得到的特征图进行Squeeze操作，得到channel级的全局特征，然后对全局特征进行Excitation操作，学习各个channel间的关系，也得到不同channel的权重，最后乘以原来的特征图得到最终特征。</p>
<p>本质上，SE模块是在channel维度上做attention或者gating操作，这种注意力机制让模型可以更加关注信息量最大的channel特征，而抑制那些不重要的channel特征。另外一点是SE模块是通用的，这意味着其可以嵌入到现有的网络架构中。</p>
</li>
<li>
<h4 id="squeeze">Squeeze</h4>
<p>原始feature map的维度为H×W×C，其中H是高度（Height），W是宽度（width），C是通道数（channel）。Squeeze就是用全局平均池化（global average pooling）把H×W×C压缩为1×1×C。H×W压缩成一维后，相当于这一维参数获得了之前H×W全局的视野，感受区域更广。公式如下：</p>
<img src="/images/senet2.jpg" alt="image-20220905093003811" style="zoom: 50%;" />
</li>
<li>
<h4 id="excitation">Excitation</h4>
<p>Sequeeze操作得到了全局描述特征，接下来需要另外一种运算来提取channel之间的关系。 用两个全连接层来学习通道间的相关性，第一个FC层起到降维的作用，然后采用ReLU激活。最后的全连接层恢复原始的维度。公式如下：</p>
<img src="/images/senet3.jpg" alt="senet3" style="zoom:50%;" />
</li>
<li>
<h4 id="reweight">Reweight</h4>
<p>将Excitation的输出权重作为经过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原始特征的重标定。</p>
</li>
<li>
<h4 id="全局平均池化">全局平均池化</h4>
<ul>
<li>
<p>思想：对于输出的每一个通道的特征图的所有像素计算一个平均值，经过全局平均池化之后就得到一个 维度=类别数的特征向量，然后直接输入到softmax层。</p>
</li>
<li>
<p>作用：代替全连接层，可接受任意尺寸的图像。</p>
</li>
<li>
<p>优点：</p>
<ol>
<li>可以更好的将类别与最后一个卷积层的特征图对应起来（每一个通道对应一种类别，这样每一张特征图都可以看成是该类别对应的类别置信图）；</li>
<li>降低参数量，全局平均池化层没有参数，可防止在该层过拟合；</li>
<li>整合了全局空间信息，对于输入图片的spatial translation更加鲁棒。</li>
</ol>
</li>
</ul>
</li>
<li>
<h4 id="结构融合">结构融合</h4>
<img src="/images/senet4.jpg" alt="senet4" style="zoom:50%;" />
<ul>
<li>上图是将SE模块嵌入到Inception结构的示例。方框旁边的维度代表该层的输出。这里使用Alobal Average Pooling作为Squeeze操作。紧接着两个Fully Connected层组成一个Bottleneck结构去建模通道间的相关性，并输出和输入特征同样数目的权重。首先将特征维度降低到输入的1/16，然后经过ReLu激活后再通过一个Fully Connected层升回到原来的维度。这样做相比直接用一个Fully Connected层的好处在于：
<ol>
<li>具有更多的非线性，可以更好地拟合通道间复杂的相关性；</li>
<li>减少了参数量和计算量。</li>
</ol>
</li>
<li>下图是SE模块嵌入到ResNet的示例。操作过程基本和SE-Inception一样，只不过是在Addition前对分支上Residual的特征进行了特征重标定。如果对Addition后主支上的特征进行重标定，由于在主干上存在0~1的scale操作，在网络较深BP优化时就会在靠近输入层容易出现梯度消散的情况，导致模型难以优化。</li>
</ul>
</li>
<li>
<h4 id="运算效率">运算效率</h4>
<p>实验表明，SE模块在参数量上的增加带来的计算量增长微乎其微，但是性能却有所提升，当然这也取决于实际应用，如果因为SE模块导致参数量增加的掠夺，可以针对性的在适当的位置削减SE模块的数量，而精度几乎不受影响。</p>
<p>以ResNet-50和SE-ResNet-50对比举例来说，SE-ResNet-50相对于ResNet-50有着10%模型参数的增长。额外的模型参数都存在于Bottleneck设计的两个Fully Connected中，由于ResNet结构中最后一个stage的特征通道数目为2048，导致模型参数有着较大的增长，实验发现移除掉最后一个stage中3个build block上的SE设定，可以将10%参数量的增长减少到2%。此时模型的精度几乎无损失。</p>
</li>
</ul>
<h2 id="mobilenet">MobileNet</h2>
<ul>
<li>
<h4 id="简介-1">简介</h4>
<p><a href="https://arxiv.org/pdf/1704.04861v1.pdf">MobileNet V1</a>是由google2016年提出，2017年发布的文章。其主要创新点在于深度可分离卷积，而整个网络实际上也是深度可分离模块的堆叠。它基于流线型架构，使用深度可分离卷积来构建轻量级深度神经网络，用于移动和嵌入式视觉应用。该网络引入了两个简单的全局超参数——宽度乘数和分辨率乘数，可以有效地在延迟和准确性之间进行权衡。</p>
</li>
<li>
<h4 id="亮点-1">亮点</h4>
<ul>
<li>深度可分离卷积的使用</li>
<li>Global Average Pooling 的使用</li>
<li>用CONV/s2（步进2的卷积）代替MaxPool+CONV</li>
</ul>
</li>
<li>
<h4 id="深度可分离卷积的使用">深度可分离卷积的使用</h4>
<ul>
<li>
<p>在进行 depthwise 卷积时只使用了一种维度为in_channels的卷积核进行特征提取（没有进行特征组合）。采用 depth-wise convolution 会有一个问题，就是导致 信息流通不畅 ，即输出的 feature map 仅包含输入的 feature map 的一部分，在这里，MobileNet 采用了 point-wise(1*1) convolution 帮助信息在通道之间流通。</p>
</li>
<li>
<p>在进行 pointwise 卷积时只使用了output_channels 种 维度为in_channels 1*1 的卷积核进行特征组合，普通卷积不同 depth 层的权重是按照 1:1:1…:1的比例进行相加的，而在这里不同 depth 层的权重是按照不同比例(可学习的参数) 进行相加的。</p>
</li>
<li>
<p>参数数量由原来的p1 = F*F*in_channels*output_channels 变为了p2 = F*F*in_channels*1 + 1*1*in_channels*output_channels，减小为原来的p2/p1 = 1/output_channels + 1/F*F，其中 F 为卷积核的尺寸，若 F = 3，参数量大约会减少到原来的 1/8-1/9 。</p>
</li>
<li>
<p>原论文中对第一层没有用此卷积，深度可分离卷积中的每一个后面都跟 BN 和 RELU。其中，Relu6=min(max(0,x),6)。</p>
<img src="/images/mobilenet1.png" alt="mobilenet1" style="zoom:50%;" />
<pre tabindex="0"><code>ReLU6就是普通的ReLU但是限制最大输出值为6（对输出值做clip），这是为了在移动端设备float16的低精度的时候，也能有很好的数值分辨率，如果对ReLU的激活范围不加限制，输出范围为0到正无穷，如果激活值非常大，分布在一个很大的范围内，则低精度的float16无法很好地精确描述如此大范围的数值，带来精度损失。
</code></pre></li>
</ul>
</li>
<li>
<h4 id="两个超参数">两个超参数</h4>
<ul>
<li>
<p>Width Multiplier(α): Thinner Models
所有层的通道数（channel） 乘以 α 参数，模型大小近似下降到原来的 α^2 倍，计算量下降到原来的 α^2倍，α ∈ ( 0 , 1 ] with typical settings of 1, 0.75, 0.5 and 0.25，降低模型的宽度。</p>
</li>
<li>
<p>Resolution Multiplier(ρ \rhoρ): Reduced Representation
输入层的 分辨率（resolution） 乘以 ρ 参数，等价于所有层的分辨率乘 ρ ，模型大小不变，计算量下降到原来的 ρ^2 倍，ρ ∈ ( 0 , 1 ]，降低输入图像的分辨率。</p>
</li>
</ul>
</li>
</ul>
<h2 id="mobilenet-v2">MobileNet V2</h2>
<ul>
<li>
<h4 id="简介-2">简介</h4>
<p><a href="https://arxiv.org/pdf/1801.04381.pdf">MobileNet V2</a>基于倒置残差结构(inverted residual structure)，原本的残差结构的主分支是有三个卷积，两个逐点卷积通道数较多，而倒置的残差结构刚好相反，中间的卷积通道数（依旧使用深度分离卷积结构）较多，旁边的较小。此外，发现去除主分支中的非线性变换是有效的，这可以保持模型表现力。论文在多个数据集上做了对比实验，验证了该架构的有效性。</p>
</li>
<li>
<h4 id="mobilenet-v1缺点">MobileNet V1缺点</h4>
<ul>
<li>
<p>结构问题</p>
<p>V1结构过于简单，没有复用图像特征，即没有concat/eltwise+ 等操作进行特征融合，而后续的一系列的ResNet，DenseNet等结构已经证明复用图像特征的有效性。</p>
</li>
<li>
<p>逐深度卷积问题：</p>
<ol>
<li>在处理低维数据（比如逐深度的卷积）时，relu函数会造成信息的丢失。</li>
<li>DW 卷积由于本身的计算特性决定它自己没有改变通道数的能力，上一层给它多少通道，它就只能输出多少通道。所以如果上一层给的通道数本身很少的话，DW 也只能很委屈的在低维空间提特征，因此效果不够好。</li>
</ol>
</li>
</ul>
</li>
<li>
<h4 id="主要改进点">主要改进点</h4>
<ul>
<li>
<p>引入倒残差（Inverted Residuals）结构，先升维再降维，增强梯度的传播，显著减少推理期间所需的内存占用。</p>
</li>
<li>
<p>去掉 Narrow layer（low dimension or depth） 后的 ReLU，保留特征多样性，增强网络的表达能力（Linear Bottlenecks）。</p>
</li>
<li>
<p>网络为全卷积的，使得模型可以适应不同尺寸的图像；使用 RELU6（最高输出为 6）激活函数，使得模型在低精度计算下具有更强的鲁棒性。</p>
</li>
<li>
<p>MobileNet V2 building block 如下所示，若需要下采样，可在 DWise 时采用步长为 2 的卷积；小网络使用小的扩张系数（expansion factor），大网络使用大一点的扩张系数（expansion factor），推荐是5~10，论文中 t = 6t。</p>
<img src="/images/mobilenet2.jpg" alt="mobilenet2" style="zoom: 50%;" />
</li>
</ul>
</li>
<li>
<h4 id="具体改进">具体改进</h4>
<img src="/images/mobilenet3.png" alt="mobilenet3" style="zoom:50%;" />
<ul>
<li>V2 去掉了第二个 PW 的激活函数改为线性激活。
论文作者称其为 Linear Bottleneck。原因如上所述是因为作者认为激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征，不如线性的效果好。</li>
<li>V2 在 DW 卷积之前新加了一个 PW 卷积。
给每个 DW 之前都配备了一个 PW，专门用来升维，定义升维系数 t=6 ，这样不管输入通道数 Cin 是多是少，经过第一个 PW 升维之后，DW 都是在相对的更高维 ( t.Cin ) 进行更好的特征提取。</li>
</ul>
</li>
</ul>
<h2 id="mobilenet-v3">MobileNet V3</h2>
<ul>
<li>
<h4 id="简介-3">简介</h4>
<p><a href="https://arxiv.org/pdf/1905.02244.pdf">MobileNet V3</a>是Google继MobileNetV2之后的又一力作，作为MobileNet系列的新成员，自然效果会提升，本文的一个亮点在于，网络的设计利用了NAS（network architecture search）算法以及NetAdapt algorithm算法。并且，本文还介绍了一些提升网络效果的trick，这些trick也提升了不少的精度以及速度。</p>
</li>
<li>
<h4 id="亮点-2">亮点</h4>
<ul>
<li>互补搜索技术组合：由资源受限的NAS执行模块级搜索，NetAdapt执行局部搜索。</li>
<li>网络结构改进：将最后一步的平均池化层前移并移除最后一个卷积层，引入h-swish激活函数。</li>
</ul>
</li>
<li>
<h4 id="具体改进措施">具体改进措施</h4>
<ul>
<li>
<p>引入SE结构</p>
<p>在bottlenet结构中加入了SE结构，并且放在了depthwise filter之后，如下图。因为SE结构会消耗一定的时间，所以作者在含有SE的结构中，将expansion layer的channel变为原来的1/4,这样作者发现，即提高了精度，同时还没有增加时间消耗。并且SE结构放在了depthwise之后。</p>
<img src="/images/mobilenet4.png" alt="image-20220912143433488" style="zoom:50%;" />
</li>
<li>
<p>修改尾部结构</p>
<p>在MobileNet V2中，avg pooling之前，存在一个1x1的卷积层，目的是提高特征图的维度，更有利于结构的预测，但是这其实带来了一定的计算量了，所以这里作者修改了，将其放在avg pooling的后面，首先利用avg pooling将特征图大小由7x7降到了1x1，降到1x1后，然后再利用1x1提高维度，这样就减少了7x7=49倍的计算量。并且为了进一步的降低计算量，作者直接去掉了前面纺锤型卷积的3x3以及1x1卷积，进一步减少了计算量，就变成了如下图第二行所示的结构，作者将其中的3x3以及1x1去掉后，精度并没有得到损失。这里降低了大约15ms的速度。</p>
<img src="/images/mobilenet5.jpg" alt="mobilenet5" style="zoom:60%;" />
</li>
<li>
<p>修改channel数量</p>
<p>修改头部卷积核channel数量，MobileNet V2中使用的是32 x 3 x 3，作者发现，其实32可以再降低一点，所以这里作者改成了16，在保证了精度的前提下，降低了3ms的速度。</p>
</li>
<li>
<p>非线性变换的改变</p>
<p>使用h-swish替换swish，swish是谷歌自家的研究成果，这次在其基础上，为速度进行了优化。swish与h-swish公式如下所示，由于sigmoid的计算耗时较长，特别是在移动端，这些耗时就会比较明显，所以作者使用ReLU6(x+3)/6来近似替代sigmoid，观察下图可以发现，其实相差不大的。利用ReLU有几点好处：1.可以在任何软硬件平台进行计算，2.量化的时候，它消除了潜在的精度损失，使用h-swish替换swith，在量化模式下回提高大约15%的效率，另外，h-swish在深层网络中更加明显。使用h-swish@16可以提高大约0.2%的精度，但是时间延长了大约20%。</p>
<img src="/images/mobilenet6.png" alt="mobilenet6" style="zoom:50%;" />
<img src="/images/mobilenet7.png" alt="mobilenet7" style="zoom:50%;" />
</li>
</ul>
</li>
<li>
<h4 id="设计方法">设计方法</h4>
<p>先使用NAS算法，优化每一个block，得到大体的网络结构，然后使用NetAdapt 算法来确定每个filter的channel的数量。</p>
<p>这里由于small model的精度以及耗时影响相对较大，mobilenet v3 large和mobilenet v3 small是分别使用NAS设计的。</p>
<p>NAS之后，可以使用NetAdapt算法设计每个layer，过程如下：</p>
<ol>
<li>先用NAS找到一个可用的结构A。</li>
<li>在A的基础上生成一系类的候选结构，并且这些候选结构消耗在一点点减少，其实就是穷举子结构。
对于每个候选结构，使用前一个模型进行初始化，（前一个模型没有的参数随机初始化就行），finetune T个epoch，得到一个大致的精度。</li>
<li>在这些候选结构中，找到最好的。</li>
<li>反复迭代，知道目标时间到达，找到最合适的结果。</li>
</ol>
</li>
</ul>
<h2 id="summary">Summary</h2>
<ul>
<li>
<h4 id="mobilenetv1--vgg的标准卷积换成深度可分离卷积">MobileNetV1 = VGG的标准卷积换成深度可分离卷积</h4>
</li>
<li>
<h4 id="mobilenetv2--v1--inverted-residuals--shortcut">MobileNetV2 = V1 + Inverted residuals + shortcut</h4>
</li>
<li>
<h4 id="mobilenetv3--v2--se-net--h-swish">MobileNetV3 = V2 + SE-NET + h-swish</h4>
</li>
</ul>
<h2 id="citation">Citation</h2>
<blockquote>
<p>北信科视觉感知研讨课程（张若琪师兄分享）</p>
<p><a href="https://github.com/hujie-frank/SENet">SENet源码</a></p>
<p><a href="https://blog.csdn.net/gaoxueyi551/article/details/120233959">SENet概览</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/347236731">论文解读《SENet》</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/345183296">一文带你深入全局平均池化</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/394975928">轻量级网络-Mobilenet系列(v1,v2,v3)</a></p>
<p><a href="https://blog.csdn.net/mzpmzk/article/details/82976871">MobileNetV1 &amp; MobileNetV2 简介</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/402766063">轻量化网络——MobileNet</a></p>
<p><a href="https://blog.csdn.net/Chunfengyanyulove/article/details/91358187">mobilenet系列之又一新成员&mdash;mobilenet-v3</a></p>
</blockquote>
<h1 id="thanks-for-reading"><em>Thanks for reading!</em></h1>

          </section>
      </p>
      
    </section>
    <footer class="post-meta">
        
            <img class="author-thumb" src="http://example.org/images/user.png" alt="Author image" nopin="nopin" />
        
        
            syui
        
        
        <time class="post-date" datetime="2022-09-02T22:27:28&#43;08:00">
            2 Sep 2022
        </time>
    </footer>
</article>

    
        
<article class="post posts">
    <header class="post-header">
        <h2 class="post-title"><a href="/posts/resnet-densenet/">ResNet &amp; DenseNet</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
          <section class="post-content">
            <h2 id="resnet">ResNet</h2>
<ul>
<li>
<h4 id="简介">简介</h4>
<p><a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a>网络在 2015年由微软实验室中的何凯明等人提出，斩获当年ImageNet竞赛中分类任务第一名，目标检测第一名。获得COCO数据集中目标检测第一名，图像分割第一名。</p>
</li>
<li>
<h4 id="亮点">亮点</h4>
<ul>
<li>超深的网络结构（超过1000层）</li>
<li>提出residual（残差结构）模块</li>
<li>使用Batch Normalization加速训练（丢弃Dropout）</li>
</ul>
</li>
<li>
<h4 id="网络结构">网络结构</h4>
<p><img src="/images/resnet7.png" alt="resnet7"></p>
<p><img src="/images/resnet8.jpg" alt="resnet8"></p>
</li>
<li>
<h4 id="为什么采用residual">为什么采用residual</h4>
<ul>
<li>
<p>在ResNet提出之前，所有的神经网络都是通过卷积层和池化层的叠加组成的。人们认为卷积层和池化层的层数越多，获取到的图片特征信息越全，学习效果也就越好。但是在实际的试验中发现，随着卷积层和池化层的叠加，不但没有出现学习效果越来越好的情况，反而出现两种问题：</p>
<ol>
<li>
<p>梯度消失和梯度爆炸。目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助。而链式法则是一个连乘的形式，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0或特别大，也就是梯度消失或爆炸。</p>
</li>
<li>
<p>退化问题。随着层数的增加，反传回来的梯度之间的相关性会越来越差，最后接近白噪声，预测效果反而越来越差。如下图所示</p>
<img src="/images/resnet.jpg" alt="resnet" style="zoom: 40%;" />
</li>
</ol>
</li>
<li>
<p>为了解决梯度消失或梯度爆炸问题，ResNet论文提出通过数据的预处理以及在网络中使用BN（Batch Normalization）层来解决。</p>
</li>
<li>
<p>为了解决深层网络中的退化问题，可以人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为残差网络 (ResNets)。ResNet论文提出了residual结构（残差结构）来减轻退化问题，下图是使用residual结构的卷积网络，可以看到随着网络的不断加深，效果并没有变差，而是变的更好了。</p>
<p><img src="/images/resnet2.jpg" alt="resnet2"></p>
</li>
</ul>
</li>
<li>
<h4 id="residual结构">residual结构</h4>
<ul>
<li>
<p>residual结构使用了一种shortcut的连接方式，也可理解为捷径。让特征矩阵隔层相加，注意F(X)和X形状要相同，所谓相加是特征矩阵相同位置上的数字进行相加。若某一较深的网络多出另一较浅网络的若干层有能力学习到恒等映射，那么这一较深网络训练得到的模型性能一定不会弱于该浅层网络。</p>
<img src="/images/resnet3.jpg" alt="resnet3" style="zoom: 40%;" />
</li>
<li>
<p>两种不同的residual：</p>
<img src="/images/resnet4.jpg" alt="resnet4" style="zoom: 40%;" />
<p>对于深层次网络，使用左边的block意味着有很大的计算量，因此右侧使用1x1卷积先将输入进行降维，然后再经过3x3卷积后，最后用1x1卷积进行升维，为了主分支上输出的特征矩阵和捷径分支上输出的特征矩阵形状相同，以便进行加法操作。（搭建深层次网络时，采用右侧的残差结构）</p>
</li>
</ul>
</li>
<li>
<h4 id="降维时的shortcut">降维时的shortcut</h4>
<p>观察ResNet18层网络，可以发现有些残差块的shortcut是实线的，而有些则是虚线的。这些虚线的shortcut上通过1×1的卷积核进行了维度处理（特征矩阵在长宽方下采样，深度方向调整成下一层残差结构所需要的channel）。</p>
</li>
<li>
<h4 id="batch-normalization">Batch Normalization</h4>
<ul>
<li>
<p>Batch Normalization是指批标准化处理，将一批数据的feature map满足均值为0，方差为1的分布规律。</p>
</li>
<li>
<p>对图像进行标准化处理，这样能够加速网络的收敛，如下图所示，对于Conv1来说输入的就是满足某一分布的特征矩阵，但对于Conv2而言输入的feature map就不一定满足某一分布规律了（注意这里所说满足某一分布规律并不是指某一个feature map的数据要满足分布规律，理论上是指整个训练样本集所对应feature map的数据要满足分布规律）。而我们Batch Normalization的目的就是使我们的feature map满足均值为0，方差为1的分布规律。计算公式如下：</p>
<img src="/images/resnet6.jpg" alt="resnet6" style="zoom:33%;" />
</li>
<li>
<p>假设feature1、feature2分别是由image1、image2经过一系列卷积池化后得到的特征矩阵，feature的channel为2，那么代表该batch的所有feature的channel1的数据，同理代表该batch的所有feature的channel2的数据。然后分别计算和的均值与方差，得到两个向量。然后在根据标准差计算公式分别计算每个channel的值（公式中的是一个很小的常量，防止分母为零的情况）。在我们训练网络的过程中，我们是通过一个batch一个batch的数据进行训练的，但是我们在预测过程中通常都是输入一张图片进行预测，此时batch size为1，如果在通过上述方法计算均值和方差就没有意义了。所以我们在训练过程中要去不断的计算每个batch的均值和方差，并使用移动平均(moving average)的方法记录统计的均值和方差，在训练完后我们可以近似认为所统计的均值和方差就等于整个训练集的均值和方差。然后在我们验证以及预测过程中，就使用统计得到的均值和方差进行标准化处理。如下图：</p>
<img src="/images/resnet5.png" alt="resnet5" style="zoom: 30%;" />
</li>
</ul>
</li>
</ul>
<h2 id="resnet-v2">ResNet V2</h2>
<ul>
<li>
<h4 id="简介-1">简介</h4>
<p><a href="https://arxiv.org/pdf/1603.05027.pdf">ResNet V2</a> 的主要贡献在于通过理论分析和大量实验证明使用恒等映射（identity mapping）作为快捷连接（skip connection）对于残差块的重要性。同时，将BN/ReLu这些activation操作挪到了Conv（真正的weights filter操作）之前，提出预激活（Pre-activation）操作，并通过与后激活（Post-activation）操作做对比实验，表明对于多层网络，使用了预激活残差单元（Pre-activation residual unit）的resnet v2都取得了比 原版本resnet更好的结果。</p>
</li>
<li>
<h4 id="亮点-1">亮点</h4>
<p>提出了新的残差模块结构：</p>
<ul>
<li>
<p>将激活函数移至旁路</p>
</li>
<li>
<p>full pre-activation</p>
</li>
</ul>
</li>
<li>
<h4 id="深度残差网络的分析">深度残差网络的分析</h4>
<p>原先resnets的残差单元可表示为：</p>
<img src="/images/resnet9.jpg" alt="resnet9" style="zoom:50%;" />
<p>如果函数f也是恒等映射，则公式可以合并为：</p>
<img src="/images/resnet10.jpg" alt="resnet10" style="zoom: 50%;" />
<p>那么任意深层的单元L与浅层单元l之间的关系可表示为：</p>
<img src="/images/resnet11.jpg" alt="resnet11" style="zoom:50%;" />
<p>该公式有以下两个特性：</p>
<ul>
<li>深层单元的特征可以由浅层单元的特征和残差函数相加得到；</li>
<li>任意深层单元的特征都可以由起始特征x0与先前所有残差函数相加得到，这与普通（plain）网络不同，普通网络的深层特征是由一系列的矩阵向量相乘得到。残差网络是连加，普通网络是连乘。</li>
</ul>
<p>该公式也带来了良好的反向传播特性，用ε表示损失函数，根据反向传播的链式传导规则，反向传播公式如下：</p>
<img src="/images/resnet12.jpg" alt="resnet12" style="zoom:45%;" />
<p>从上述公式中可以看出，反向传播也是两条路径，其中之一直接将信息回传，另一条会经过所有的带权重层。另外可以注意到第二项的值在一个mini-batch中不可能一直是-1，也就是说回传的梯度不会消失，不论网络中的权值的值再小都不会发生梯度消失现象。</p>
</li>
<li>
<h4 id="恒等映射的重要性">恒等映射的重要性</h4>
<p>考虑恒等映射的重要性。假设将恒等映射改为
$$
h(x_l) = \lambda_l x_l
$$
则有：</p>
<img src="/images/resnet13.jpg" alt="resnet13" style="zoom:50%;" />
<p>递归调用得：</p>
<img src="/images/resnet14.jpg" alt="resnet14" style="zoom: 40%;" />
<p>其中F表示将标量合并到残差函数中，反向传播公式如下：</p>
<img src="/images/resnet15.jpg" alt="resnet15" style="zoom:40%;" />
<p>与之前的公式不同，上公式的第一个加法项由因子 ∏i=lL−1λi 进行调节。对于一个极深的网络( L极大)，考虑第一个连乘的项，如果所有的 λ 都大于1，那么这一项会指数级增大；如果所有 λ 都小于1，那么这一项会很小甚至消失，会阻断来自shortcut的反向传播信号，并迫使其流过权重层。本文通过实验证明这种方式会对模型优化造成困难。另外其他不同形式的变换映射也都会阻碍信号的有效传播，进而影响训练进程。</p>
</li>
<li>
<h4 id="不同形式激活函数的效果">不同形式激活函数的效果</h4>
<ul>
<li>
<p><strong>BN after addition</strong> 效果比基准差，BN 层移到相加操作后面会阻碍信号传播，一个明显的现象就是训练初期误差下降缓慢。</p>
</li>
<li>
<p><strong>ReLU before addition</strong> 这样组合的话残差函数分支的输出就一直保持非负，这会影响到模型的表示能力，而实验结果也表明这种组合比基准差。</p>
</li>
<li>
<p><strong>Post-activation or pre-activation</strong> 原来的设计中相加操作后面还有一个ReLU激活函数，这个激活函数会影响到残差单元的两个分支，现在将它移到残差函数分支上，快捷连接分支不再受到影响。具体操作如下图：</p>
<img src="/images/resnet16.jpg" alt="ressnet16" style="zoom:50%;" />
<p>根据激活函数与相加操作的位置关系，我们称之前的组合方式为“后激活（post-activation）”，现在新的组合方式称之为“预激活（pre-activation）”。预激活方式又可以分为两种：只将ReLU放在前面，或者将ReLU和BN都放到前面，实验结果显示full pre-activation的效果要更好。</p>
</li>
<li>
<p>使用预激活有两个方面的<strong>优点</strong>：</p>
<ol>
<li>f变为恒等映射，使得网络更易于优化；</li>
<li>使用BN作为预激活可以加强对模型的正则化。</li>
</ol>
</li>
<li>
<p><strong>Ease of optimization</strong> 这在训练1001层残差网络时尤为明显。使用原来设计的网络在起始阶段误差下降很慢，因为f是ReLU激活函数，当信号为负时会被截断，使模型无法很好地逼近期望函数；而使用预激活网络中的f是恒等映射，信号可以在不同单元直接直接传播。本文使用的1001层网络优化速度很快，并且得到了最低的误差。</p>
</li>
<li>
<p>f为ReLU对浅层残差网络的影响并不大。本文认为是当网络经过一段时间的训练之后权值经过适当的调整，使得单元输出基本都是非负，此时f不再对信号进行截断。但是截断现象在超过1000层的网络中经常发生。</p>
</li>
<li>
<p><strong>Reducing overfitting</strong> 使用了预激活的网络的训练误差稍高，但却得到更低的测试误差，本文推测这是BN层的正则化效果所致。在原始残差单元中，尽管BN对信号进行了标准化，但是它很快就被合并到捷径连接(shortcut)上，组合的信号并不是被标准化的。这个非标准化的信号又被用作下一个权重层的输入。与之相反，本文的预激活（pre-activation）版本的模型中，权重层的输入总是标准化的。</p>
</li>
</ul>
</li>
</ul>
<h2 id="resnext">ResNeXt</h2>
<ul>
<li>
<h4 id="简介-2">简介</h4>
<p><a href="https://arxiv.org/pdf/1611.05431.pdf">ResNeXt</a>是ResNet和Inception的结合体，不同于Inception v4的是，ResNext不需要人工设计复杂的Inception结构细节，而是每一个分支都采用相同的拓扑结构。ResNeXt的本质是分组卷积（Group Convolution），通过变量基数（Cardinality）来控制组的数量。分组卷机是普通卷积和深度可分离卷积的一个折中方案，即每个分支产生的Feature Map的通道数为 n(n&gt;1) 。</p>
</li>
<li>
<h4 id="神经网络标准范式">神经网络标准范式</h4>
<p><strong>split-transform-merge</strong></p>
<img src="/images/resnet17.png" alt="resnet17" style="zoom:40%;" />
<p>如上图所示，先将输入分配到多路，然后每一路进行转换，最后再把所有支路的结果融合。</p>
</li>
<li>
<h4 id="基本结构">基本结构</h4>
<img src="/images/resnet18.jpg" alt="resnet18" style="zoom: 40%;" />
</li>
<li>
<h4 id="具体结构">具体结构</h4>
<img src="/images/resnet19.png" alt="resnet19" style="zoom:40%;" />
<p>类似ResNet，作者选择了很简单的基本结构，每一组C个不同的分支都进行相同的简单变换，上面是ResNeXt-50（32x4d）的配置清单，32指进入网络的第一个ResNeXt基本结构的分组数量C（即基数）为32，4d表示depth即每一个分组的通道数为4（所以第一个基本结构输入通道数为128）。</p>
<p>可以看到ResNet-50和ResNeXt-50（32x4d）拥有相同的参数，但是精度却更高。具体实现上，因为1x1卷积可以合并，就合并了，代码更简单，并且效率更高。</p>
<p>得益于精心设计的复杂的网络结构，ResNet-Inception v2可能效果会更好一点，但是ResNeXt的网络结构更简单，可以防止对于特定数据集的过拟合。而且更简单的网络意味着在用于自己的任务的时候，自定义和修改起来更简单。</p>
</li>
<li>
<h4 id="总结">总结</h4>
<p>ResNeXt提出了一种介于普通卷积核深度可分离卷积的这种策略：分组卷积，他通过控制分组的数量（基数）来达到两种策略的平衡。分组卷积的思想是源自Inception，不同于Inception的需要人工设计每个分支，ResNeXt的每个分支的拓扑结构是相同的。最后再结合残差网络，得到的便是最终的ResNeXt。</p>
<p>从上面的分析中我们可以看书ResNeXt的结构非常简单，但是其在ImageNet上取得了由于相同框架的残差网络，也算是Inception直接助攻了一把吧。</p>
<p>ResNeXt确实比Inception V4的超参数更少，但是他直接废除了Inception的囊括不同感受野的特性仿佛不是很合理，在更多的环境中我们发现Inception V4的效果是优于ResNeXt的。类似结构的ResNeXt的运行速度应该是优于Inception V4的，因为ResNeXt的相同拓扑结构的分支的设计是更符合GPU的硬件设计原则。</p>
</li>
</ul>
<h2 id="densenet">DenseNet</h2>
<ul>
<li>
<h4 id="简介-3">简介</h4>
<p><a href="https://arxiv.org/pdf/1608.06993.pdf">DenseNet</a>模型的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接。DenseNet脱离了加深网络层数(ResNet)和加宽网络结构(Inception)来提升网络性能的定式思维，从特征的角度考虑，通过特征重用和旁路(Bypass)设置，既大幅度减少了网络的参数量，又在一定程度上缓解了gradient vanishing问题的产生。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能，DenseNet也因此斩获CVPR 2017的最佳论文奖。</p>
</li>
<li>
<h4 id="亮点-2">亮点</h4>
<ul>
<li>由于密集连接方式，DenseNet提升了梯度的反向传播，使得网络更容易训练。由于每层可以直达最后的误差信号，实现了隐式的“deep supervision”。</li>
<li>参数更小且计算更高效，这有点违反直觉，由于DenseNet是通过concat特征来实现短路连接，实现了特征重用，并且采用较小的growth rate，每个层所独有的特征图是比较小的。</li>
<li>由于特征复用，最后的分类器使用了低级特征。</li>
</ul>
</li>
<li>
<h4 id="设计理念">设计理念</h4>
<p>相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。下图1为ResNet网络的连接机制，作为对比，下图2为DenseNet的密集连接机制。可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的），并作为下一层的输入。对于一个L层的网络，DenseNet共包含L(L+1)/2个连接，相比ResNet，这是一种密集连接。而DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别。</p>
<img src="/images/densenet1.png" alt="densenet1" style="zoom:60%;" />
<img src="/images/densenet2.png" alt="densenet2" style="zoom:60%;" />
</li>
<li>
<h4 id="网络结构-1">网络结构</h4>
<img src="/images/densenet3.jpg" alt="densenet3" style="zoom:60%;" />
<p>假设输入为一个图片 X0 , 经过一个L层的神经网络，其中第i层的非线性变换记为 Hi ()，Hi ()可以是多种函数操作的累加如BN、ReLU、Pooling或Conv等。第i层的特征输出记作 Xi。</p>
<p>传统卷积前馈神经网络将第i层的输出Xi作为i+1层的输入，可以写作Xi = Hi ( Xi−1 )。ResNet增加了旁路连接，可以写作Xl = Xl ( Xl−1 )+ Xl−1。</p>
<p>ResNet的一个最主要的优势便是梯度可以流经恒等函数来到达靠前的层，但恒等映射和非线性变换输出的叠加方式是相加，这在一定程度上破坏了网络中的信息流。</p>
<p>为了进一步优化信息流的传播，DenseNet提出了上图的网络结构。如图所示，第i层的输入不仅与i-1层的输出相关，还有所有之前层的输出有关，记作:</p>
<p>Xl = Hl ([ X0 , X1 ,…, Xl−1 ])，</p>
<p>其中[]代表concatenation(拼接)，既将 X0 到 Xl−1 层的所有输出feature map按Channel组合在一起。这里所用到的非线性变换H为BN+ReLU+ Conv(3×3)的组合。</p>
</li>
<li>
<h4 id="denseblocktransition">DenseBlock+Transition</h4>
<p>CNN网络一般要经过Pooling或者stride&gt;1的Conv来降低特征图的大小，而DenseNet的密集连接方式需要特征图大小保持一致。为了解决这个问题，DenseNet网络中使用DenseBlock+Transition的结构。</p>
<ul>
<li>DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。</li>
<li>Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。</li>
</ul>
<p><img src="/images/densenet4.jpg" alt="densenet4"></p>
</li>
<li>
<h4 id="总结-1">总结</h4>
<ul>
<li>
<p>一些较早层提取出的特征仍可能被较深层直接使用。</p>
</li>
<li>
<p>即使是Transition layer也会使用到之前Denseblock中所有层的特征。</p>
</li>
<li>
<p>第2-3个Denseblock中的层对之前Transition layer利用率很低,说明transition layer输出大量冗余特征.这也为DenseNet-BC提供了证据支持,既Compression的必要性。</p>
</li>
<li>
<p>最后的分类层虽然使用了之前Denseblock中的多层信息,但更偏向于使用最后几个feature map的特征,说明在网络的最后几层,某些high-level的特征可能被产生。</p>
</li>
</ul>
</li>
</ul>
<h2 id="citation">Citation</h2>
<blockquote>
<p>北信科视觉感知研讨课程（陈露元同学分享）</p>
<p><a href="https://blog.csdn.net/qq_45649076/article/details/120494328">ResNet详解</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/31852747">你必须要知道CNN模型：ResNet</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/410444216">Backbone论文-ResNet v2 解读</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/32913695">深度学习——分类之ResNeXt</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/51075096">ResNeXt详解</a></p>
<p><a href="https://blog.csdn.net/qq_44766883/article/details/112011420">详解DenseNet(密集连接的卷积网络)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/43057737">DenseNet详解</a></p>
</blockquote>
<h1 id="thanks-for-reading"><em>Thanks for reading!</em></h1>

          </section>
      </p>
      
    </section>
    <footer class="post-meta">
        
            <img class="author-thumb" src="http://example.org/images/user.png" alt="Author image" nopin="nopin" />
        
        
            syui
        
        
        <time class="post-date" datetime="2022-08-26T10:54:08&#43;08:00">
            26 Aug 2022
        </time>
    </footer>
</article>

    
        
<article class="post posts">
    <header class="post-header">
        <h2 class="post-title"><a href="/posts/alexnet-vgg-googlenet/">AlexNet &amp; VGG &amp; GoogLeNet</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
          <section class="post-content">
            <h2 id="alexnet">AlexNet</h2>
<ul>
<li>
<h4 id="简介">简介</h4>
<p><a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">AlexNet</a>是2012年ISLVRC 2012（ImageNet Large Scale Visual Recognition Challenge）竞赛的冠军网络，分类准确率由传统的 70%+提升到 80%+。由Hinton和他的学生Alex Krizhevsky设计。也是在那年之后，深度学习开始迅速发展。</p>
</li>
<li>
<h4 id="亮点">亮点</h4>
<ul>
<li>首次使用GPU进行模型训练。</li>
<li>使用了ReLU激活函数，而不是传统的Sigmoid激活函数以及Tanh激活函数。主要原因是ReLU函数在进行梯度下降的计算过程中能显著加快训练过程，也就是非饱和的线性的激活函数要快于sigmoid和tanh等饱和的非线性激活函数的收敛速度（在这篇论文中并没有谈及ReLU函数对于梯度消失问题的解决，只是从收敛速度上论述的）。</li>
<li>使用了LRN局部响应归一化（Local Response Normalization，LRN）。AlexNet中提出局部响应归一化层（LRN）,是跟在激活或池化之后的一种为了提高准确度的技术方法。LRN的主要思想就是对局部神经元的神经活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈更小的神经元，从而也在一定程度上增强了泛化能力。</li>
<li>在全连接层的前两层中使用了Dropout随机失活神经元操作，以减少过拟合。</li>
</ul>
</li>
<li>
<h4 id="模型架构">模型架构</h4>
<p><img src="/images/alexnet.png" alt="alexnet"></p>
</li>
<li>
<h4 id="relu激活函数">ReLU激活函数</h4>
<p>针对sigmoid梯度饱和导致训练收敛慢的问题，在AlexNet中引入了ReLU。ReLU是一个分段线性函数，小于等于0则输出为0；大于0的则恒等输出。相比于sigmoid，ReLU有以下有特点：</p>
<ul>
<li>计算开销小。sigmoid的正向传播有指数运算，倒数运算，而ReLu是线性输出；反向传播中，sigmoid有指数运算，而ReLU有输出的部分，导数始终为1。</li>
<li>梯度饱和问题。</li>
<li>稀疏性。ReLU会使一部跟神经元的输出为0，这样造就了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。</li>
</ul>
</li>
<li>
<h4 id="dropout">Dropout</h4>
<ul>
<li>
<p>过拟合的根本原因是特征维度过多，模型过于复杂，参数过多，训练数据过少，噪声过多，导致模型完美拟合训练集，但缺乏泛化能力，对新数据的测试集预测结果差。</p>
</li>
<li>
<p>Dropout的使用有效地缓解了模型复杂度提升导致的过拟合。Dropout通过修改神经网络本身结构来实现，对于某一层的神经元，通过定义的概率将神经元置为0，这个神经元就不参与前向和后向传播，就如同在网络中被删除了一样，同时保持输入层与输出层神经元的个数不变，然后按原有的学习方法进行参数更新。在下一次迭代中，又重新随机删除一些神经元（置为0），直至训练结束。</p>
</li>
<li>
<p>Dropout应该算是AlexNet中一个很大的创新，现在神经网络中的必备结构之一。Dropout也可以看成是一种模型组合，每次生成的网络结构都不一样，通过组合多个模型的方式能够有效地减少过拟合，Dropout只需要两倍的训练时间即可实现模型组合（类似取平均）的效果，非常高效。</p>
</li>
</ul>
</li>
</ul>
<h2 id="vgg">VGG</h2>
<ul>
<li>
<h4 id="简介-1">简介</h4>
<p><a href="https://arxiv.org/abs/1409.1556">VGG</a>是由牛津大学视觉几何小组（Visual Geometry Group, VGG）提出的一种深层卷积网络结构，赢得了 2014 年 ILSVRC 分类任务的亚军（冠军由 GoogLeNet 夺得）和定位任务的冠军。VGG可以看成是加深版本的AlexNet，都是conv layer + FC layer。该网络是在ILSVRC 2014上的相关工作，主要工作是证明了增加网络的深度能够在一定程度上影响网络最终的性能。</p>
</li>
<li>
<h4 id="亮点-1">亮点</h4>
<ul>
<li>证明了增加网络的深度能够在一定程度上影响网络最终的性能。</li>
<li>小卷积核和小池化核，卷积核全部替换为3*3（极少用了1*1），相比AlexNet的3*3池化核，VGG全部为2*2的池化核。</li>
<li>层数更深特征图更宽，由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，计算量的增加放缓。</li>
<li>全连接层换成卷积层，网络测试阶段将训练阶段的三个全连接替换为三个卷积，测试重用训练时的参数，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入。</li>
</ul>
</li>
<li>
<h4 id="模型架构-1">模型架构</h4>
<p><img src="/images/VGG.jpg" alt="VGG"></p>
</li>
<li>
<h4 id="网络特点">网络特点</h4>
<ul>
<li>
<p>两个3x3的卷积堆叠获得的感受野大小，相当一个5x5的卷积；而3个3x3卷积的堆叠获取到的感受野相当于一个7x7的卷积。好处如下：</p>
<ol>
<li>多层卷积引入了多次非线性变换，可以增加网络的非线性表达能力。</li>
<li>两层3*3卷积核的参数量比一层5*5卷积核的参数量更少，可以看作对网络做了相应的正则化。</li>
</ol>
<p><img src="/images/33.png" alt="33"></p>
</li>
<li>
<p>网络测试阶段将训练阶段的三个全连接替换为三个卷积，使得测试阶段对输入图片尺寸没有限制。使用全连接层和卷积层的区别：</p>
<p>​		全连接层：通道数相同的情况下，特征图大小一样，才能保证与全连接层中的神经元数目相同。</p>
<p>​		卷积层：通道数目来决定最终的分类输出数目，不需要特征图大小相同。</p>
<p><img src="/images/conv.jpg" alt="conv"></p>
</li>
</ul>
</li>
</ul>
<h2 id="googlenet">GoogLeNet</h2>
<ul>
<li>
<h4 id="简介-2">简介</h4>
<p><a href="https://arxiv.org/pdf/1409.4842.pdf">GoogLeNet</a>是2014年Christian Szegedy提出的一种全新的深度学习结构，在这之前的AlexNet、VGG等结构都是通过增大网络的深度来获得更好的训练效果，但层数的增加会带来很多负作用，比如过拟合、梯度消失、梯度爆炸等。inception的提出则从另一种角度来提升训练结果：更高效的利用计算资源，在相同的计算量下能提取到更多的特征，从而提升训练结果。</p>
</li>
<li>
<h4 id="前代网络缺陷">前代网络缺陷</h4>
<p>​	LeNet，AlexNet，VGG这些串联网络通过增大网络的深度来获得更好的训练效果，主要存在以下缺陷：</p>
<ul>
<li>提取的特征图尺度单一。串联网络中，每一层级的卷积核都是固定尺寸的，只能提取固定尺度的特征。基于这种尺度单一的特征图构建的模型鲁棒性不强，泛化能力差。</li>
<li>参数量巨大，且难以将梯度传递到网络顶层。虽然多层小卷积核堆叠取得大卷积核能够减少参数量，但杯水车薪，网络越深也越容易发生梯度消失，使得网络难以训练。</li>
</ul>
</li>
<li>
<h4 id="解决方案及亮点">解决方案及亮点</h4>
<ul>
<li>使用1x1的卷积来进行升降维。1×1卷积核的重要作用：降维或升维、跨通道信息交融、减少参数量、增加模型深度并提高非线性表达能力。</li>
<li>与直觉相一致，即视觉信息应该在不同的尺度上进行处理再聚合。</li>
<li>引入稀疏特性、将全连接层换成稀疏连接，利用稀疏矩阵分解成密集矩阵计算的原理来加快收敛速度：
<ol>
<li>空间（spatial）维度，只对图像的某一部分进行卷积，而不是对整个图像进行卷积。</li>
<li>特征（feature）维度，在多个尺寸上进行卷积再聚合，把相关性强的特征聚集到一起，每一种尺寸的卷积只输出256个特征中的一部分。</li>
</ol>
</li>
</ul>
</li>
<li>
<h4 id="各版本inception改进">各版本inception改进</h4>
<ul>
<li>
<p><strong>inception-v1</strong></p>
<ol>
<li>将多尺度的卷积层、池化层提取的特征图拼接输入下一层，提升模型多尺度特征提取能力；</li>
<li>利用1*1卷积对输入通道降维，减少模型参数量，并引入更多的非线性，提升模型泛化能力。</li>
</ol>
</li>
<li>
<p><strong>inception-v2</strong></p>
<ol>
<li>将Inception-v1中的大尺寸卷积核替换为多个小尺寸卷积核的堆叠，减少模型参数量，增加网络深度，提升模型拟合复杂分布的能力；</li>
<li>如果在训练模型时，每一批次的训练样本分布差别较大，那么网络的每一层都要重新去适应学习新的数据分布，这非常影响网络的训练速度，因此Inception-v2对卷积层进行了BatchNormalizatioin，有效地加速了网络训练过程，也能够消除梯度弥散；</li>
<li>引入了辅助分类器，即在靠近输出层的层级上引入旁路分类器，来解决梯度无法反向传递非常深的问题，辅助分类器只在训练时使能，测试时不开启辅助分类器；</li>
<li>结合了对称卷积与非对称卷积，增加了网络宽度。</li>
</ol>
</li>
<li>
<p><strong>inception-v3</strong></p>
<p>inception-v3的改进基于以下几点优化思路：</p>
<ol>
<li>避免特征表示瓶颈。尤其在网络前端，要避免严重压缩导致的瓶颈：特征表示尺寸应该从输入到输出端温和地减少，避免特征信息大量丢失。</li>
<li>高维信息更适合在网络的局部处理。在卷积网络中逐步增加非线性激活函数响应可以解耦更多的特征，加速网络训练。</li>
<li>空间聚合可以通过低维嵌入，不会导致网络表示能力的下降。在进行大尺寸卷积之前，在空间聚合前先对输入信息进行降维处理，可以加速训练。</li>
<li>平衡好网路的深度和宽度。通过平衡网络每层滤波器的个数和网络的层数可以使网络达到最佳性能。并行增加网络的深度和宽度时，模型性能提升最大。</li>
</ol>
<p>改进点如下：</p>
<ol>
<li>优化算法使用RMSProp替代SGD。</li>
<li>使用Label Smoothing Regularization方法。LSR是一种通过在输出y中加入噪声，对模型进行约束，降低模型过拟合的方法。</li>
<li>将大尺寸卷积核替换成小尺寸卷积核。</li>
<li>对辅助分类器中的全连接层进行BatchNormalization。</li>
</ol>
</li>
<li>
<p><strong>inception-v4</strong></p>
<p>Inception-v4基本沿袭了Inception-v2/v3的设计，它的各个模块在结构上更加统一。</p>
</li>
<li>
<p><strong>inception-ResNet</strong></p>
<p>Inception-ResNet借鉴了何凯明的残差连接思想，将Inception模块与残差连接进行结合。残差连接是指浅层特征通过另外一条支路直接加到高层特征中，达到特征复用的目的，可以有效避免深层网络的梯度弥散问题。</p>
</li>
</ul>
</li>
</ul>
<h2 id="self-thinking">Self-thinking</h2>
<ul>
<li>
<h4 id="提高深度神经网络性能方法">提高深度神经网络性能方法</h4>
<p>增加网络大小，包括深度和宽度，这是最直接的办法，也是最简单最安全的办法，尤其是当给定大量带标签的训练数据时。但是缺点明显：增加网络中训练的参数（导致过拟合），增加计算量。</p>
</li>
<li>
<h4 id="上述问题的解决方案">上述问题的解决方案</h4>
<p>从全连接层或卷积层完全转变到稀疏连接的结构。Arora等人的开创性工作表明如果数据集的概率分布可以用一个大的、非常稀疏的深度神经网络来表示，那么可以通过分析最后一层激活函数的相关统计量（均值方差等）和聚合有着高度相关输出的神经元来逐层生成最优的网络拓扑，这与赫布原理相仿——神经元一起发射且连接在一起。</p>
</li>
<li>
<h4 id="inception系列引发的思考好的深度网络设计原则">Inception系列引发的思考（好的深度网络设计原则）</h4>
<ul>
<li>逐层构造网络。如果数据集的概率分布能够被一个神经网络所表达，那么构造这个网络的最佳方法是逐层构筑网络，即将上一层高度相关的节点连接在一起。几乎所有效果好的深度网络都具有这一点，不管AlexNet，VGG堆叠多个卷积，googLeNet堆叠多个inception模块，还是ResNet堆叠多个resblock。</li>
<li>稀疏的结构。人脑的神经元连接就是稀疏的，因此大型神经网络的合理连接方式也应该是稀疏的。稀疏的结构对于大型神经网络至关重要，可以减轻计算量并减少过拟合。 卷积操作（局部连接，权值共享）本身就是一种稀疏的结构，相比于全连接网络结构是很稀疏的。</li>
<li>符合Hebbian原理。Cells that fire together, wire together. 一起发射的神经元会连在一起。 相关性高的节点应该被连接而在一起。</li>
</ul>
<p>而inception中 1×1的卷积恰好可以融合以上三者。一层可能会有多个卷积核，在同一个位置但在不同通道的卷积核输出结果相关性极高。一个1×1的卷积核可以很自然的把这些相关性很高，在同一个空间位置，但不同通道的特征结合起来。而其它尺寸的卷积核（3×3，5×5）可以保证特征的多样性，因此也可以适量使用。</p>
</li>
<li>
<h4 id="总结">总结</h4>
<ul>
<li>运用赫布原理和多尺度并行处理：从FC层转变到稀疏结构。</li>
<li>Inception模块的集成：使用1×1卷积核进行降维再用5×5卷积核升维。一个googlenet里包含九个Inceptionv1模块。</li>
<li>全局平均池化替代FC：减少参数量。做迁移学习更方便，用在弱监督学习和半监督学习中非常的可靠。</li>
</ul>
</li>
</ul>
<h2 id="citation">Citation</h2>
<blockquote>
<p>北信科视觉感知研讨课程（高丹阳师姐分享）</p>
<p><a href="https://blog.csdn.net/Katrina_ALi/article/details/102854403">AlexNet论文阅读总结及代码</a></p>
<p><a href="https://blog.csdn.net/DreamBro/article/details/121068023">VGG网络详解</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/41423739">一文读懂VGG网络</a></p>
<p><a href="https://blog.csdn.net/qq_40635082/article/details/123106503">GoogLeNet论文翻译及解读</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/478197016">深度学习-GoogLeNet</a></p>
</blockquote>
<h1 id="thanks-for-reading"><em>Thanks for reading!</em></h1>

          </section>
      </p>
      
    </section>
    <footer class="post-meta">
        
            <img class="author-thumb" src="http://example.org/images/user.png" alt="Author image" nopin="nopin" />
        
        
            syui
        
        
        <time class="post-date" datetime="2022-08-20T18:00:30&#43;08:00">
            20 Aug 2022
        </time>
    </footer>
</article>

    
        
<article class="post posts">
    <header class="post-header">
        <h2 class="post-title"><a href="/posts/%E6%96%B0%E4%B8%80%E4%BB%A3%E8%A7%86%E8%A7%89%E6%84%9F%E7%9F%A5%E7%BB%BC%E8%BF%B0/">新一代视觉感知综述</a></h2>
    </header>
    <section class="post-excerpt">
      <p>
          <section class="post-content">
            <h2 id="经典篇">经典篇</h2>
<ul>
<li>
<h4 id="visual-perception-视觉感知">Visual Perception (视觉感知)</h4>
<ul>
<li>
<p>Characterizing edges (特征边缘)：边缘是指图像强度函数中急剧变化的位置。</p>
<p><img src="/images/image-20220819165142510.png" alt="image-20220819165142510"></p>
</li>
<li>
<p>图像滤波：在尽量保留图像细节特征的条件下对目标图像的噪声进行抑制，是图像处理中不可缺少的操作，其处理效果的好坏将直接影响到后续图像处理和分析的有效性和可靠性。</p>
</li>
<li>
<p>canny边缘检测算法：</p>
<ol>
<li>用高斯滤波器对图像进行平滑处理；</li>
<li>利用一阶偏导算子找到图像灰度沿水平方向和垂直方向的偏导数，并求出梯度的幅值和方位；</li>
<li>非极大值抑制，找到局部梯度最大值；</li>
<li>用双阈值算法检测和链接边缘，凡大于高阈值T1的一定是边缘，凡小于低阈值T2的一定不是边缘，若既大于低阈值又小于高阈值，则要看这个像素的邻接像素中是否有大于高阈值的边缘像素，若有则是边缘，否则不是。</li>
</ol>
</li>
</ul>
</li>
<li>
<h4 id="feature-extraction-特征提取">Feature Extraction (特征提取)</h4>
</li>
<li>
<h4 id="bag-of-words-bow-词袋模型">Bag of Words, BoW (词袋模型)</h4>
</li>
</ul>
<h2 id="现代篇">现代篇</h2>
<ul>
<li>
<h4 id="cnn-卷积神经网络">CNN (卷积神经网络)</h4>
</li>
<li>
<h4 id="vision-transformer-视觉自注意力网络">Vision Transformer (视觉自注意力网络)</h4>
</li>
</ul>
<h2 id="前沿篇">前沿篇</h2>
<ul>
<li>
<h4 id="xai-可解释性ai">XAI (可解释性AI)</h4>
</li>
<li>
<h4 id="contrastive-pretraining-对比预训练">Contrastive Pretraining (对比预训练）</h4>
</li>
<li>
<h4 id="multimodal-fusion-多模态融合">Multimodal Fusion (多模态融合）</h4>
</li>
</ul>

          </section>
      </p>
      
    </section>
    <footer class="post-meta">
        
            <img class="author-thumb" src="http://example.org/images/user.png" alt="Author image" nopin="nopin" />
        
        
            syui
        
        
        <time class="post-date" datetime="2022-08-13T23:47:44&#43;08:00">
            13 Aug 2022
        </time>
    </footer>
</article>

    

    <nav class="pagination" role="navigation">
	
	<span class="page-number">Page 1 of 2</span>
	
	    <a class="older-posts" href="/page/2/">Older Posts &rarr;</a>
	
</nav>


</main>

    <footer class="site-footer clearfix">
        <section class="copyright"><a href="">Hzb&#39;s Study Blog</a> </section>
        
        <section class="poweredby">Proudly generated by <a class="icon-hugo" href="https://gohugo.io">HUGO</a>, with <a class="icon-theme" href="https://github.com/syui/hugo-theme-air">hugo-theme-air</a> theme</section>
        
    </footer>
    </div>
    <script type="text/javascript" src="http://example.org/js/jquery.js"></script>
    <script type="text/javascript" src="http://example.org/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="http://example.org/js/index.js"></script>
    <script src="http://example.org/js/particles.min.js"></script>
    <script src="http://example.org/js/particles.js"></script>  

</body>
</html>

